{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate Data Quality Checks with Great Expectations\n",
    "**Introduction**: In this activity, you will learn how to automate data quality checks using the Great Expectations framework. This includes setting up expectations and generating validation reports.\n",
    "\n",
    "### Task 1: Setup and Initial Expectations\n",
    "\n",
    "1. Objective: Set up Great Expectations and create initial expectations for a dataset.\n",
    "2. Steps:\n",
    "    - Install Great Expectations using pip.\n",
    "    - Initialize a data context.\n",
    "    - Create basic expectations on a sample dataset.\n",
    "    - Eg., Implement a basic setup and expectation for column presence and type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Validate Datasets and Generate Reports\n",
    "\n",
    "1. Objective: Validate a dataset against defined expectations and generate a report.\n",
    "2. Steps:\n",
    "    - Execute the validation process on the dataset.\n",
    "    - Review the validation results and generate a report.\n",
    "    - Eg., Validate completeness and consistency expectations, and view the results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Advanced Expectations and Scheduling\n",
    "\n",
    "1. Objective: Create advanced expectations for conditional checks and automate the validation.\n",
    "2. Steps:\n",
    "    - Define advanced expectations based on complex conditions.\n",
    "    - Use scheduling tools to automate periodic checks.\n",
    "    - E.g., an expectation that customer IDs must be unique and schedule a daily check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataContext' from 'great_expectations.data_context' (/home/vscode/.local/lib/python3.10/site-packages/great_expectations/data_context/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mge\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mgreat_expectations\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata_context\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataContext\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mshutil\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataContext' from 'great_expectations.data_context' (/home/vscode/.local/lib/python3.10/site-packages/great_expectations/data_context/__init__.py)"
     ]
    }
   ],
   "source": [
    "# This script demonstrates how to automate data quality checks using Great Expectations.\n",
    "# It covers setting up a data context, defining expectations, validating data,\n",
    "# and generating data quality reports.\n",
    "\n",
    "# --- Prerequisites ---\n",
    "# Before running this code, you need to install Great Expectations.\n",
    "# Open your terminal or command prompt and run:\n",
    "# pip install great_expectations pandas\n",
    "\n",
    "import pandas as pd\n",
    "import great_expectations as ge\n",
    "from great_expectations.data_context import DataContext\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# --- Task 1: Setup and Initial Expectations ---\n",
    "\n",
    "print(\"--- Task 1: Setting up Great Expectations and Initial Expectations ---\")\n",
    "\n",
    "# Define a directory for the Great Expectations project.\n",
    "# This will create a 'great_expectations' subdirectory in your current working directory.\n",
    "ge_project_dir = \"my_ge_project\"\n",
    "\n",
    "# Clean up previous GE project if it exists for a fresh start\n",
    "if os.path.exists(ge_project_dir):\n",
    "    print(f\"Removing existing Great Expectations project directory: {ge_project_dir}\")\n",
    "    shutil.rmtree(ge_project_dir)\n",
    "\n",
    "# Initialize a Great Expectations data context.\n",
    "# This creates the 'great_expectations' directory with configuration files.\n",
    "# The `ge.data_context.DataContext.create()` method is used for programmatic initialization.\n",
    "# For interactive setup, you would typically run `great_expectations init` in the terminal.\n",
    "print(f\"Initializing Great Expectations data context in '{ge_project_dir}'...\")\n",
    "context = ge.data_context.DataContext.create(project_dir=ge_project_dir)\n",
    "print(\"Great Expectations data context initialized.\")\n",
    "\n",
    "# Create a sample dataset using Pandas\n",
    "print(\"\\nCreating a sample dataset...\")\n",
    "data = {\n",
    "    'customer_id': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 11], # Duplicate customer_id 1\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve', 'Frank', 'Grace', 'Heidi', 'Ivan', 'Judy', 'Alice', 'Kyle'],\n",
    "    'email': ['alice@example.com', 'bob@example.com', 'charlie@example.com', 'david@example.com', 'eve@example.com',\n",
    "              'frank@example.com', 'grace@example.com', 'heidi@example.com', 'ivan@example.com', 'judy@example.com',\n",
    "              'alice@example.com', 'kyle@example.com'],\n",
    "    'age': [25, 30, 35, 40, 28, 32, 29, 45, 22, 38, 25, None], # Missing age for Kyle\n",
    "    'city': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'Phoenix', 'Philadelphia', 'San Antonio', 'San Diego', 'Dallas', 'San Jose', 'New York', 'Austin'],\n",
    "    'registration_date': ['2023-01-15', '2022-03-20', '2023-07-01', '2021-11-10', '2023-02-28',\n",
    "                          '2022-09-05', '2023-04-12', '2021-06-30', '2023-05-18', '2022-01-01',\n",
    "                          '2023-01-15', '2023-10-25'],\n",
    "    'order_count': [5, 12, 8, 20, 3, 15, 7, 25, 2, 10, 5, 6],\n",
    "    'is_active': [True, True, False, True, True, False, True, True, True, False, True, True]\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "print(df.head())\n",
    "\n",
    "# Add the DataFrame to a Great Expectations Data Source.\n",
    "# Here we use an in-memory Pandas DataFrame as a data source.\n",
    "# In a real-world scenario, you might connect to a database or files.\n",
    "datasource_name = \"my_pandas_datasource\"\n",
    "data_asset_name = \"customer_data\"\n",
    "\n",
    "# Configure the datasource in the context\n",
    "context.add_datasource(\n",
    "    name=datasource_name,\n",
    "    class_name=\"PandasDatasource\",\n",
    "    batch_spec_passthrough={\"reader_method\": \"dataframe\"},\n",
    ")\n",
    "\n",
    "# Get a batch of data from the datasource\n",
    "# This is how Great Expectations interacts with your data for defining expectations.\n",
    "batch_request = ge.core.batch_request.BatchRequest(\n",
    "    datasource_name=datasource_name,\n",
    "    data_asset_name=data_asset_name,\n",
    "    data_connector_name=\"default_runtime_data_connector\",\n",
    "    data_connector_query={\"batch_filter_parameters\": {\"batch_data\": df}},\n",
    ")\n",
    "validator = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=\"customer_data_suite\"\n",
    ")\n",
    "\n",
    "print(\"\\nCreating basic expectations for the dataset...\")\n",
    "# Expectation 1: `customer_id` column exists\n",
    "validator.expect_column_to_exist(\"customer_id\")\n",
    "# Expectation 2: `name` column exists\n",
    "validator.expect_column_to_exist(\"name\")\n",
    "# Expectation 3: `age` column exists\n",
    "validator.expect_column_to_exist(\"age\")\n",
    "# Expectation 4: `email` column exists\n",
    "validator.expect_column_to_exist(\"email\")\n",
    "# Expectation 5: `city` column exists\n",
    "validator.expect_column_to_exist(\"city\")\n",
    "# Expectation 6: `registration_date` column exists\n",
    "validator.expect_column_to_exist(\"registration_date\")\n",
    "# Expectation 7: `order_count` column exists\n",
    "validator.expect_column_to_exist(\"order_count\")\n",
    "# Expectation 8: `is_active` column exists\n",
    "validator.expect_column_to_exist(\"is_active\")\n",
    "\n",
    "\n",
    "# Expectation 9: `customer_id` column values are integers\n",
    "validator.expect_column_values_to_be_of_type(\"customer_id\", \"int\")\n",
    "# Expectation 10: `name` column values are strings\n",
    "validator.expect_column_values_to_be_of_type(\"name\", \"str\")\n",
    "# Expectation 11: `age` column values are integers (allowing nulls for now)\n",
    "validator.expect_column_values_to_be_of_type(\"age\", \"int\")\n",
    "# Expectation 12: `email` column values are strings\n",
    "validator.expect_column_values_to_be_of_type(\"email\", \"str\")\n",
    "# Expectation 13: `city` column values are strings\n",
    "validator.expect_column_values_to_be_of_type(\"city\", \"str\")\n",
    "# Expectation 14: `registration_date` column values match a date format\n",
    "validator.expect_column_values_to_match_strftime_format(\"registration_date\", \"%Y-%m-%d\")\n",
    "# Expectation 15: `order_count` column values are integers\n",
    "validator.expect_column_values_to_be_of_type(\"order_count\", \"int\")\n",
    "# Expectation 16: `is_active` column values are boolean\n",
    "validator.expect_column_values_to_be_of_type(\"is_active\", \"bool\")\n",
    "\n",
    "\n",
    "# Save the expectation suite\n",
    "validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "print(\"Initial expectation suite 'customer_data_suite' saved.\")\n",
    "\n",
    "# --- Task 2: Validate Datasets and Generate Reports ---\n",
    "\n",
    "print(\"\\n--- Task 2: Validating Datasets and Generating Reports ---\")\n",
    "\n",
    "# Execute the validation process\n",
    "print(\"Executing validation process...\")\n",
    "# You can define a Checkpoint to run validations and build Data Docs.\n",
    "# A Checkpoint is a configuration that bundles a set of validations.\n",
    "checkpoint_name = \"customer_data_checkpoint\"\n",
    "context.add_checkpoint(\n",
    "    name=checkpoint_name,\n",
    "    validator=validator, # Use the validator created earlier\n",
    "    # You can also specify batch_request here if you want to reuse the checkpoint\n",
    "    # for different batches of data.\n",
    "    action_list=[\n",
    "        {\n",
    "            \"name\": \"store_validation_result\",\n",
    "            \"action_class\": \"StoreValidationResultAction\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"store_evaluation_parameter_metrics\",\n",
    "            \"action_class\": \"StoreEvaluationParametersAction\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"update_data_docs\",\n",
    "            \"action_class\": \"UpdateDataDocsAction\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "\n",
    "# Run the checkpoint\n",
    "validation_result = context.run_checkpoint(checkpoint_name=checkpoint_name)\n",
    "\n",
    "# Review the validation results\n",
    "print(\"\\nValidation Results Summary:\")\n",
    "if validation_result.success:\n",
    "    print(\"Validation successful! All expectations passed.\")\n",
    "else:\n",
    "    print(\"Validation failed. Some expectations did not pass.\")\n",
    "    for result in validation_result.results:\n",
    "        if not result.success:\n",
    "            print(f\"  Failed Expectation: {result.expectation_config.expectation_type} for column '{result.expectation_config.column}'\")\n",
    "            print(f\"    Details: {result.result}\")\n",
    "\n",
    "# Generate and open a Data Docs report\n",
    "# Data Docs are automatically built when the `UpdateDataDocsAction` is included in the checkpoint.\n",
    "# You can open them manually or programmatically.\n",
    "print(\"\\nGenerating and opening Data Docs report...\")\n",
    "# This command will open the Data Docs in your default web browser.\n",
    "# In a real scenario, you might host these Data Docs on a web server.\n",
    "context.open_data_docs()\n",
    "print(\"Data Docs report generated and opened in your browser (if supported by your environment).\")\n",
    "\n",
    "\n",
    "# --- Task 3: Advanced Expectations and Scheduling ---\n",
    "\n",
    "print(\"\\n--- Task 3: Advanced Expectations and Scheduling ---\")\n",
    "\n",
    "# Define advanced expectations based on complex conditions.\n",
    "# Reload the validator to add more expectations\n",
    "validator_advanced = context.get_validator(\n",
    "    batch_request=batch_request,\n",
    "    expectation_suite_name=\"customer_data_suite_advanced\" # Create a new suite for advanced expectations\n",
    ")\n",
    "\n",
    "print(\"\\nAdding advanced expectations...\")\n",
    "\n",
    "# Expectation 17: `customer_id` must be unique\n",
    "validator_advanced.expect_column_values_to_be_unique(\"customer_id\")\n",
    "\n",
    "# Expectation 18: `age` column values are between 18 and 99\n",
    "validator_advanced.expect_column_values_to_be_between(\"age\", min_value=18, max_value=99)\n",
    "\n",
    "# Expectation 19: `email` column values match a regex pattern for email format\n",
    "validator_advanced.expect_column_values_to_match_regex(\"email\", r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\")\n",
    "\n",
    "# Expectation 20: `order_count` is never negative\n",
    "validator_advanced.expect_column_values_to_be_between(\"order_count\", min_value=0, max_value=None)\n",
    "\n",
    "# Expectation 21: Conditional check: If `is_active` is True, then `order_count` must be greater than 0.\n",
    "# This requires a custom expectation or a more complex approach.\n",
    "# For simplicity, let's use a common pattern:\n",
    "# Expectation: `order_count` column values are greater than 0 if `is_active` is True\n",
    "# This is a more complex expectation that might require a custom `Expectation` class\n",
    "# or a more direct data manipulation for validation.\n",
    "# For demonstration, we'll use a simpler form:\n",
    "validator_advanced.expect_column_pair_values_to_be_in_set(\n",
    "    column_A=\"is_active\",\n",
    "    column_B=\"order_count\",\n",
    "    value_pairs=[(True, count) for count in range(1, 1000)] # Assuming order_count won't exceed 999\n",
    ")\n",
    "# A more robust way for conditional logic might involve filtering the DataFrame first\n",
    "# and then applying expectations to the filtered subset.\n",
    "# Example:\n",
    "# active_customers_df = df[df['is_active'] == True]\n",
    "# if not active_customers_df.empty:\n",
    "#     active_customer_validator = context.get_validator(\n",
    "#         batch_request=ge.core.batch_request.BatchRequest(\n",
    "#             datasource_name=datasource_name,\n",
    "#             data_asset_name=\"active_customer_data\",\n",
    "#             data_connector_name=\"default_runtime_data_connector\",\n",
    "#             data_connector_query={\"batch_filter_parameters\": {\"batch_data\": active_customers_df}},\n",
    "#         ),\n",
    "#         expectation_suite_name=\"active_customer_suite\"\n",
    "#     )\n",
    "#     active_customer_validator.expect_column_values_to_be_between(\"order_count\", min_value=1, max_value=None)\n",
    "#     active_customer_validator.save_expectation_suite(discard_failed_expectations=False)\n",
    "\n",
    "\n",
    "# Save the advanced expectation suite\n",
    "validator_advanced.save_expectation_suite(discard_failed_expectations=False)\n",
    "print(\"Advanced expectation suite 'customer_data_suite_advanced' saved.\")\n",
    "\n",
    "# Run validation with advanced expectations\n",
    "checkpoint_advanced_name = \"customer_data_advanced_checkpoint\"\n",
    "context.add_checkpoint(\n",
    "    name=checkpoint_advanced_name,\n",
    "    validator=validator_advanced,\n",
    "    action_list=[\n",
    "        {\n",
    "            \"name\": \"store_validation_result\",\n",
    "            \"action_class\": \"StoreValidationResultAction\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"store_evaluation_parameter_metrics\",\n",
    "            \"action_class\": \"StoreEvaluationParametersAction\",\n",
    "        },\n",
    "        {\n",
    "            \"name\": \"update_data_docs\",\n",
    "            \"action_class\": \"UpdateDataDocsAction\",\n",
    "        },\n",
    "    ],\n",
    ")\n",
    "validation_result_advanced = context.run_checkpoint(checkpoint_name=checkpoint_advanced_name)\n",
    "\n",
    "print(\"\\nAdvanced Validation Results Summary:\")\n",
    "if validation_result_advanced.success:\n",
    "    print(\"Advanced validation successful! All expectations passed.\")\n",
    "else:\n",
    "    print(\"Advanced validation failed. Some expectations did not pass.\")\n",
    "    for result in validation_result_advanced.results:\n",
    "        if not result.success:\n",
    "            print(f\"  Failed Expectation: {result.expectation_config.expectation_type} for column '{result.expectation_config.column}'\")\n",
    "            print(f\"    Details: {result.result}\")\n",
    "\n",
    "context.open_data_docs() # Open Data Docs again to see the advanced suite results\n",
    "\n",
    "# --- Automate Periodic Checks (Conceptual) ---\n",
    "\n",
    "print(\"\\n--- Automating Periodic Checks (Conceptual) ---\")\n",
    "print(\"Great Expectations validations can be automated using various scheduling tools.\")\n",
    "print(\"Here are some common approaches:\")\n",
    "\n",
    "print(\"\\n1. Cron Jobs (Linux/macOS) / Task Scheduler (Windows):\")\n",
    "print(\"   You can set up a cron job to run this Python script at a specific interval (e.g., daily).\")\n",
    "print(\"   Example cron entry (runs daily at 2 AM):\")\n",
    "print(\"   0 2 * * * /usr/bin/python3 /path/to/your/script.py\")\n",
    "\n",
    "print(\"\\n2. Apache Airflow:\")\n",
    "print(\"   For more complex data pipelines, Airflow is an excellent choice.\")\n",
    "print(\"   You would define a DAG (Directed Acyclic Graph) that includes a task to run your Great Expectations checkpoint.\")\n",
    "print(\"   Example Airflow DAG snippet:\")\n",
    "print(\"   from airflow import DAG\")\n",
    "print(\"   from airflow.operators.bash import BashOperator\")\n",
    "print(\"   from datetime import datetime\")\n",
    "print(\"\\n   with DAG('data_quality_check', start_date=datetime(2023, 1, 1), schedule_interval='@daily') as dag:\")\n",
    "print(\"       run_ge_validation = BashOperator(\")\n",
    "print(\"           task_id='run_ge_validation',\")\n",
    "print(\"           bash_command='python /path/to/your/script.py', # Or a more specific GE command\")\n",
    "print(\"       )\")\n",
    "\n",
    "print(\"\\n3. Prefect / Dagster / Other Orchestration Tools:\")\n",
    "print(\"   Similar to Airflow, these tools provide robust frameworks for orchestrating data workflows,\")\n",
    "print(\"   including tasks for data validation with Great Expectations.\")\n",
    "\n",
    "print(\"\\n4. CI/CD Pipelines (e.g., Jenkins, GitLab CI, GitHub Actions):\")\n",
    "print(\"   Integrate Great Expectations validation into your CI/CD pipeline to ensure data quality\")\n",
    "print(\"   before deploying new data or models to production.\")\n",
    "print(\"   For example, run a validation step after a data transformation job completes.\")\n",
    "\n",
    "print(\"\\nTo truly automate, you would replace the direct script execution with calls from these schedulers.\")\n",
    "print(\"The Data Docs can then be hosted on a web server for easy access to validation reports.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
