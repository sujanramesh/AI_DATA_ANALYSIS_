{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Data Quality SLAs\n",
    "### Data Completeness\n",
    "**Description**: Set an SLA that ensures that 95% of data fields in your dataset are filled (non-null values). Practice by checking a dataset of your choice and calculate its completeness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overall data completeness: 80.00%\n",
      "SLA Failed: Data completeness below threshold.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def check_data_completeness(df, threshold=0.95):\n",
    "    completeness = df.notnull().mean()  # fraction of non-null per column\n",
    "    overall_completeness = completeness.mean()  # average across all columns\n",
    "    print(f\"Overall data completeness: {overall_completeness:.2%}\")\n",
    "    if overall_completeness >= threshold:\n",
    "        print(\"SLA Passed: Data completeness meets the threshold.\")\n",
    "    else:\n",
    "        print(\"SLA Failed: Data completeness below threshold.\")\n",
    "    return overall_completeness >= threshold\n",
    "\n",
    "# Example dataset\n",
    "data = {\n",
    "    'A': [1, 2, None, 4, 5],\n",
    "    'B': [5, None, 7, 8, 9],\n",
    "    'C': ['x', 'y', 'z', None, 'w']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "check_data_completeness(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Timeliness:\n",
    "**Description**: Establish an SLA that specifies that data should be integrated and processed within 24 hours of acquisition. Monitor the data pipeline for timeliness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TimedeltaIndex' object has no attribute 'dt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 31\u001b[0m\n\u001b[1;32m     20\u001b[0m acquisition_times \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime([\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025-05-25 08:00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025-05-25 09:30:00\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025-05-25 11:00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     24\u001b[0m ])\n\u001b[1;32m     25\u001b[0m processing_times \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime([\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025-05-26 07:00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# within 24 hrs\u001b[39;00m\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025-05-26 10:00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# within 24 hrs\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2025-05-27 12:00:00\u001b[39m\u001b[38;5;124m\"\u001b[39m   \u001b[38;5;66;03m# > 24 hrs, late\u001b[39;00m\n\u001b[1;32m     29\u001b[0m ])\n\u001b[0;32m---> 31\u001b[0m \u001b[43mcheck_data_timeliness\u001b[49m\u001b[43m(\u001b[49m\u001b[43macquisition_times\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocessing_times\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 9\u001b[0m, in \u001b[0;36mcheck_data_timeliness\u001b[0;34m(acquisition_times, processing_times, max_delay_hours)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;03macquisition_times: pd.Series of datetime when data was acquired\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mprocessing_times: pd.Series of datetime when data was processed\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      8\u001b[0m delays \u001b[38;5;241m=\u001b[39m processing_times \u001b[38;5;241m-\u001b[39m acquisition_times\n\u001b[0;32m----> 9\u001b[0m delays_in_hours \u001b[38;5;241m=\u001b[39m \u001b[43mdelays\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[38;5;241m.\u001b[39mtotal_seconds() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m3600\u001b[39m\n\u001b[1;32m     10\u001b[0m timely \u001b[38;5;241m=\u001b[39m delays_in_hours \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m max_delay_hours\n\u001b[1;32m     11\u001b[0m timely_pct \u001b[38;5;241m=\u001b[39m timely\u001b[38;5;241m.\u001b[39mmean()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'TimedeltaIndex' object has no attribute 'dt'"
     ]
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def check_data_timeliness(acquisition_times, processing_times, max_delay_hours=24):\n",
    "    \"\"\"\n",
    "    acquisition_times: pd.Series of datetime when data was acquired\n",
    "    processing_times: pd.Series of datetime when data was processed\n",
    "    \"\"\"\n",
    "    delays = processing_times - acquisition_times\n",
    "    delays_in_hours = delays.dt.total_seconds() / 3600\n",
    "    timely = delays_in_hours <= max_delay_hours\n",
    "    timely_pct = timely.mean()\n",
    "    print(f\"Timely processing: {timely_pct:.2%} within {max_delay_hours} hours\")\n",
    "    if timely_pct >= 1.0:  # SLA: 100% processed within 24 hrs\n",
    "        print(\"SLA Passed: All data processed within SLA window.\")\n",
    "    else:\n",
    "        print(\"SLA Failed: Some data processed beyond SLA window.\")\n",
    "    return timely_pct >= 1.0\n",
    "\n",
    "# Example data\n",
    "acquisition_times = pd.to_datetime([\n",
    "    \"2025-05-25 08:00:00\",\n",
    "    \"2025-05-25 09:30:00\",\n",
    "    \"2025-05-25 11:00:00\"\n",
    "])\n",
    "processing_times = pd.to_datetime([\n",
    "    \"2025-05-26 07:00:00\",  # within 24 hrs\n",
    "    \"2025-05-26 10:00:00\",  # within 24 hrs\n",
    "    \"2025-05-27 12:00:00\"   # > 24 hrs, late\n",
    "])\n",
    "\n",
    "check_data_timeliness(acquisition_times, processing_times)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Consistency:\n",
    "**Description**: Define an SLA for maintaining consistency across various related datasets. Implement a check to ensure that 99% of data entries are consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data consistency: 75.00%\n",
      "SLA Failed: Data consistency below threshold.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_data_consistency(df1, df2, key_columns, threshold=0.99):\n",
    "    \"\"\"\n",
    "    Check consistency between two datasets on key columns.\n",
    "    Consistency = fraction of matching entries on keys.\n",
    "    \"\"\"\n",
    "    # Merge datasets on key columns (inner join)\n",
    "    merged = pd.merge(df1, df2, on=key_columns, how='inner', suffixes=('_1', '_2'))\n",
    "\n",
    "    # Check matching rows on all columns except keys (assumes same columns)\n",
    "    data_cols = [c for c in df1.columns if c not in key_columns]\n",
    "    consistent_rows = (merged[[col + '_1' for col in data_cols]] ==\n",
    "                       merged[[col + '_2' for col in data_cols]].values).all(axis=1)\n",
    "    consistency_ratio = consistent_rows.mean()\n",
    "\n",
    "    print(f\"Data consistency: {consistency_ratio:.2%}\")\n",
    "    if consistency_ratio >= threshold:\n",
    "        print(\"SLA Passed: Data consistency meets threshold.\")\n",
    "    else:\n",
    "        print(\"SLA Failed: Data consistency below threshold.\")\n",
    "    return consistency_ratio >= threshold\n",
    "\n",
    "# Example datasets\n",
    "df1 = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'value': [10, 20, 30, 40]\n",
    "})\n",
    "\n",
    "df2 = pd.DataFrame({\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'value': [10, 20, 35, 40]  # one mismatch on id=3\n",
    "})\n",
    "\n",
    "check_data_consistency(df1, df2, key_columns=['id'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
