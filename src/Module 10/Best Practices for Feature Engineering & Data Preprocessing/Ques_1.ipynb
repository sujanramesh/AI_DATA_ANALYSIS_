{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Handling Missing Values - Simple Imputation\n",
    "**Description**: Given a dataset with missing values, impute the missing values using the mean for numerical features and the mode for categorical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Feature Scaling - Min-Max Normalization\n",
    "**Description**: Normalize a numerical feature using Min-Max scaling to a range [0, 1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Handling Missing Values - Drop Missing Values\n",
    "**Description**: Remove rows with missing values from a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Feature Scaling - Standardization\n",
    "**Description**: Standardize a numerical feature to have zero mean and unit variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "from scipy.stats import kstest # Import kstest for Kolmogorov-Smirnov test\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score # Import metrics for adversarial validation\n",
    "import shap # Import SHAP library for explainability\n",
    "from sklearn.ensemble import RandomForestClassifier # Using RandomForest for SHAP demonstration\n",
    "from scipy.stats import chi2_contingency # Import for Chi-squared test\n",
    "\n",
    "\n",
    "# Download necessary NLTK data (if not already downloaded)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('stemmers/porter')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt') # punkt is needed for tokenization, which stemming implicitly uses\n",
    "\n",
    "\n",
    "print(\"Simulating concept drift and preparing visualization...\")\n",
    "\n",
    "# --- Parameters for Concept Drift Simulation ---\n",
    "num_time_periods = 50  # Number of simulated time periods\n",
    "samples_per_period = 100 # Number of data points in each time period\n",
    "base_probability = 0.3  # Initial probability of the binary target (e.g., P(Y=1))\n",
    "drift_magnitude = 0.01 # How much the probability changes per period\n",
    "drift_start_period = 15 # When the drift starts\n",
    "drift_end_period = 40   # When the drift ends\n",
    "\n",
    "# --- Simulate Concept Drift ---\n",
    "# This list will store the true probability of the target variable for each period\n",
    "true_probabilities = []\n",
    "# This list will store the observed frequency of the target variable for each period\n",
    "observed_frequencies = []\n",
    "\n",
    "for i in range(num_time_periods):\n",
    "    current_probability = base_probability\n",
    "\n",
    "    # Introduce concept drift between drift_start_period and drift_end_period\n",
    "    if drift_start_period <= i < drift_end_period:\n",
    "        # Gradually increase the probability over time\n",
    "        current_probability = base_probability + (i - drift_start_period) * drift_magnitude\n",
    "        # Ensure probability stays within [0, 1]\n",
    "        current_probability = np.clip(current_probability, 0.0, 1.0)\n",
    "    elif i >= drift_end_period:\n",
    "        # Keep the probability constant after the drift ends (at its peak value)\n",
    "        current_probability = base_probability + (drift_end_period - drift_start_period) * drift_magnitude\n",
    "        current_probability = np.clip(current_probability, 0.0, 1.0)\n",
    "\n",
    "    true_probabilities.append(current_probability)\n",
    "\n",
    "    # Simulate binary data for the current period based on current_probability\n",
    "    # np.random.binomial(n, p, size) generates n trials with probability p, repeated 'size' times.\n",
    "    # Here, n=1 for a single binary outcome, and size=samples_per_period for multiple samples.\n",
    "    simulated_data = np.random.binomial(1, current_probability, samples_per_period)\n",
    "\n",
    "    # Calculate the observed frequency (proportion of 1s) in the simulated data\n",
    "    observed_frequency = np.mean(simulated_data)\n",
    "    observed_frequencies.append(observed_frequency)\n",
    "\n",
    "# --- Visualize the Concept Drift ---\n",
    "plt.figure(figsize=(12, 6)) # Set the figure size for better readability\n",
    "plt.plot(range(num_time_periods), true_probabilities, label='True Probability (Simulated Drift)', color='blue', linestyle='--', marker='o', markersize=4)\n",
    "plt.plot(range(num_time_periods), observed_frequencies, label='Observed Frequency', color='red', alpha=0.7, marker='x', markersize=4)\n",
    "\n",
    "plt.title('Simulated Concept Drift in Binary Target Variable Over Time', fontsize=16)\n",
    "plt.xlabel('Time Period', fontsize=12)\n",
    "plt.ylabel('Probability / Frequency of Target (Y=1)', fontsize=12)\n",
    "plt.grid(True, linestyle=':', alpha=0.7) # Add a subtle grid\n",
    "plt.legend(fontsize=10) # Display the legend\n",
    "plt.ylim(0, 1) # Ensure y-axis is between 0 and 1 for probabilities\n",
    "plt.xticks(np.arange(0, num_time_periods + 1, 5)) # Set x-axis ticks for clarity\n",
    "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "plt.show() # Display the plot\n",
    "\n",
    "print(\"\\nConcept drift simulation and visualization complete. The plot shows how the distribution of the binary target variable changes over time.\")\n",
    "\n",
    "\n",
    "# --- New Section: Detecting & Handling Imbalanced Data ---\n",
    "print(\"\\n\\n--- Detecting & Handling Imbalanced Data: Visualizing Class Imbalance ---\")\n",
    "\n",
    "# Define the URL for the Credit Card Fraud Detection dataset\n",
    "# This dataset is known for its highly imbalanced classes (very few fraudulent transactions).\n",
    "credit_card_fraud_url = \"https://raw.githubusercontent.com/mlg-ulb/fpg/master/datasets/creditcard.csv\"\n",
    "\n",
    "print(\"Loading the Credit Card Fraud Detection dataset...\")\n",
    "try:\n",
    "    # Load the dataset into a pandas DataFrame\n",
    "    # The dataset is comma-separated and has a header.\n",
    "    df_fraud = pd.read_csv(credit_card_fraud_url)\n",
    "    print(\"Credit Card Fraud Detection dataset loaded successfully.\")\n",
    "    print(\"\\nFirst 5 rows of the fraud detection dataset:\")\n",
    "    print(df_fraud.head())\n",
    "\n",
    "    print(\"\\n--- Initial Class Distribution ---\")\n",
    "    # The target variable is 'Class' (0: Non-Fraud, 1: Fraud)\n",
    "    class_distribution = df_fraud['Class'].value_counts()\n",
    "    print(\"Absolute counts of each class:\")\n",
    "    print(class_distribution)\n",
    "\n",
    "    # Calculate and print the percentage of each class\n",
    "    class_percentage = df_fraud['Class'].value_counts(normalize=True) * 100\n",
    "    print(\"\\nPercentage of each class:\")\n",
    "    print(class_percentage.round(2))\n",
    "\n",
    "    # --- Visualize Class Imbalance ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x='Class', data=df_fraud, palette='viridis')\n",
    "    plt.title('Class Distribution in Original Credit Card Fraud Dataset', fontsize=14)\n",
    "    plt.xlabel('Class (0: Non-Fraud, 1: Fraud)', fontsize=12)\n",
    "    plt.ylabel('Number of Transactions', fontsize=12)\n",
    "    plt.xticks(ticks=[0, 1], labels=['Non-Fraud (0)', 'Fraud (1)'])\n",
    "    plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Class imbalance visualized. The plot clearly shows the disproportionate number of non-fraudulent vs. fraudulent transactions.\")\n",
    "\n",
    "    print(\"\\n--- Applying Random Undersampling to Balance the Dataset ---\")\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df_fraud.drop('Class', axis=1)\n",
    "    y = df_fraud['Class']\n",
    "\n",
    "    # Identify indices of majority and minority classes\n",
    "    # Class 0 is the majority class, Class 1 is the minority class\n",
    "    fraud_indices = np.array(df_fraud[df_fraud.Class == 1].index)\n",
    "    non_fraud_indices = np.array(df_fraud[df_fraud.Class == 0].index)\n",
    "\n",
    "    # Determine the number of samples in the minority class (fraudulent transactions)\n",
    "    number_of_fraud_samples = len(fraud_indices)\n",
    "    print(f\"Number of fraudulent transactions (minority class): {number_of_fraud_samples}\")\n",
    "\n",
    "    # Randomly select 'number_of_fraud_samples' from the non-fraudulent class\n",
    "    # np.random.choice is used to pick random indices without replacement\n",
    "    random_non_fraud_indices = np.random.choice(\n",
    "        non_fraud_indices,\n",
    "        number_of_fraud_samples,\n",
    "        replace=False # Important: do not pick the same sample multiple times\n",
    "    )\n",
    "    print(f\"Randomly selected {len(random_non_fraud_indices)} non-fraudulent samples.\")\n",
    "\n",
    "    # Concatenate the indices of the minority class and the randomly selected majority class\n",
    "    undersampled_indices = np.concatenate([fraud_indices, random_non_fraud_indices])\n",
    "\n",
    "    # Create the undersampled DataFrame\n",
    "    df_undersampled = df_fraud.loc[undersampled_indices]\n",
    "\n",
    "    print(\"\\n--- Class Distribution After Random Undersampling ---\")\n",
    "    # Check the class distribution in the undersampled dataset\n",
    "    undersampled_class_distribution = df_undersampled['Class'].value_counts()\n",
    "    print(\"Absolute counts of each class after undersampling:\")\n",
    "    print(undersampled_class_distribution)\n",
    "\n",
    "    # Calculate and print the percentage of each class after undersampling\n",
    "    undersampled_class_percentage = df_undersampled['Class'].value_counts(normalize=True) * 100\n",
    "    print(\"\\nPercentage of each class after undersampling:\")\n",
    "    print(undersampled_class_percentage.round(2))\n",
    "\n",
    "    # --- Visualize Class Distribution After Undersampling ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x='Class', data=df_undersampled, palette='viridis')\n",
    "    plt.title('Class Distribution After Random Undersampling', fontsize=14)\n",
    "    plt.xlabel('Class (0: Non-Fraud, 1: Fraud)', fontsize=12)\n",
    "    plt.ylabel('Number of Transactions', fontsize=12)\n",
    "    plt.xticks(ticks=[0, 1], labels=['Non-Fraud (0)', 'Fraud (1)'])\n",
    "    plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Class distribution after random undersampling visualized. The classes are now balanced.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing the Credit Card Fraud Detection dataset: {e}\")\n",
    "    print(\"Please ensure the dataset URL is correct and accessible, and that necessary libraries are installed.\")\n",
    "\n",
    "\n",
    "# --- New Section: Ensuring Consistency Across Training & Inference Datasets: Feature Scaling ---\n",
    "print(\"\\n\\n--- Ensuring Consistency Across Training & Inference Datasets: Feature Scaling ---\")\n",
    "\n",
    "print(\"Generating a synthetic dataset for demonstrating feature scaling...\")\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "# We'll create a simple dataset with two features for demonstration\n",
    "np.random.seed(42) # for reproducibility\n",
    "num_samples = 200\n",
    "data = {\n",
    "    'Feature1': np.random.normal(loc=50, scale=10, size=num_samples),\n",
    "    'Feature2': np.random.normal(loc=1000, scale=200, size=num_samples)\n",
    "}\n",
    "df_scaling = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nOriginal synthetic dataset (first 5 rows):\")\n",
    "print(df_scaling.head())\n",
    "print(\"\\nOriginal synthetic dataset descriptive statistics:\")\n",
    "print(df_scaling.describe().round(2))\n",
    "\n",
    "# Split the dataset into training and 'new' (inference) data\n",
    "# This simulates having a dataset on which you train your model, and then new data comes in for prediction.\n",
    "X_train, X_new_inference = train_test_split(df_scaling, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "print(f\"New inference data shape: {X_new_inference.shape}\")\n",
    "\n",
    "print(\"\\n--- Applying StandardScaler to Training Data ---\")\n",
    "# Initialize the StandardScaler\n",
    "# StandardScaler transforms data to have a mean of 0 and a standard deviation of 1.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "# The .fit() method calculates the mean and standard deviation from the training data.\n",
    "# The .transform() method then applies these calculated values to scale the data.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Convert the scaled array back to a DataFrame for better readability\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "\n",
    "print(\"\\nScaled training data (first 5 rows):\")\n",
    "print(X_train_scaled_df.head())\n",
    "print(\"\\nScaled training data descriptive statistics (should have mean ~0, std ~1):\")\n",
    "print(X_train_scaled_df.describe().round(2))\n",
    "\n",
    "\n",
    "print(\"\\n--- Applying the SAME Scaler to New Inference Data ---\")\n",
    "# Apply the *already fitted* scaler to the new inference data\n",
    "# It is CRUCIAL to use the same scaler (i.e., the one fitted on training data)\n",
    "# to transform new data. This ensures consistency in scaling.\n",
    "X_new_inference_scaled = scaler.transform(X_new_inference)\n",
    "\n",
    "# Convert the scaled array back to a DataFrame\n",
    "X_new_inference_scaled_df = pd.DataFrame(X_new_inference_scaled, columns=X_new_inference.columns)\n",
    "\n",
    "print(\"\\nScaled new inference data (first 5 rows):\")\n",
    "print(X_new_inference_scaled_df.head())\n",
    "print(\"\\nScaled new inference data descriptive statistics:\")\n",
    "print(X_new_inference_scaled_df.describe().round(2))\n",
    "\n",
    "print(\"\\nFeature scaling demonstration complete. Notice that the same scaler object was used for both training and new inference data to maintain consistency.\")\n",
    "print(\"This ensures that your model, trained on scaled data, receives new data in the same expected format.\")\n",
    "\n",
    "\n",
    "# --- New Section: Bias & Fairness in Data: Bias Mitigation Techniques (Reweighing) ---\n",
    "print(\"\\n\\n--- Bias & Fairness in Data: Bias Mitigation Techniques (Reweighing) ---\")\n",
    "\n",
    "# Define the URL for the Adult Income dataset\n",
    "adult_income_url_reweigh = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "# Define the column names for the Adult Income dataset\n",
    "column_names_reweigh = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "\n",
    "print(\"Loading the Adult Income dataset for bias mitigation...\")\n",
    "try:\n",
    "    df_adult = pd.read_csv(adult_income_url_reweigh, sep=\", \", header=None, names=column_names_reweigh, na_values=[\"?\"], engine='python')\n",
    "    print(\"Adult Income dataset loaded successfully.\")\n",
    "\n",
    "    # Clean the 'sex' and 'income' columns by stripping whitespace\n",
    "    for col in ['sex', 'income']:\n",
    "        if col in df_adult.columns and df_adult[col].dtype == 'object':\n",
    "            df_adult[col] = df_adult[col].str.strip()\n",
    "\n",
    "    # Drop rows with any missing values for a clean demonstration\n",
    "    df_adult.dropna(inplace=True)\n",
    "    print(f\"Dataset shape after dropping missing values: {df_adult.shape}\")\n",
    "\n",
    "    print(\"\\n--- Original Distribution of Income by Gender ---\")\n",
    "    original_distribution = pd.crosstab(df_adult['sex'], df_adult['income'], margins=True, normalize=False)\n",
    "    print(\"Absolute counts:\")\n",
    "    print(original_distribution)\n",
    "\n",
    "    original_percentage = pd.crosstab(df_adult['sex'], df_adult['income'], margins=True, normalize='index') * 100\n",
    "    print(\"\\nRow-wise percentages (Income distribution within each Gender):\")\n",
    "    print(original_percentage.round(2))\n",
    "\n",
    "    # --- Apply Reweighing Technique ---\n",
    "    # Goal: Make the proportion of income classes (especially '>50K') more similar across genders.\n",
    "    # Calculate overall probabilities of income classes\n",
    "    p_y_le50k = df_adult['income'].value_counts(normalize=True)['<=50K']\n",
    "    p_y_gt50k = df_adult['income'].value_counts(normalize=True)['>50K']\n",
    "\n",
    "    # Initialize a new column for weights\n",
    "    df_adult['sample_weight'] = 1.0\n",
    "\n",
    "    # Calculate weights based on the formula: weight(s, y) = P(Y=y) / P(Y=y | S=s)\n",
    "    # This aims to make the joint distribution P(S, Y) proportional to P(S)P(Y)\n",
    "    # effectively making S and Y independent in the weighted dataset.\n",
    "\n",
    "    # Iterate through unique combinations of 'sex' and 'income'\n",
    "    for sex_val in df_adult['sex'].unique():\n",
    "        for income_val in df_adult['income'].unique():\n",
    "            # Probability of income_val given sex_val: P(Y=y | S=s)\n",
    "            p_y_given_s = len(df_adult[(df_adult['sex'] == sex_val) & (df_adult['income'] == income_val)]) / len(df_adult[df_adult['sex'] == sex_val])\n",
    "\n",
    "            # Overall probability of income_val: P(Y=y)\n",
    "            p_y = df_adult['income'].value_counts(normalize=True)[income_val]\n",
    "\n",
    "            # Calculate weight for this specific group\n",
    "            # Avoid division by zero if a group has no samples\n",
    "            if p_y_given_s > 0:\n",
    "                weight = p_y / p_y_given_s\n",
    "            else:\n",
    "                weight = 0 # Assign 0 weight if the group doesn't exist\n",
    "\n",
    "            # Assign calculated weight to the corresponding samples\n",
    "            df_adult.loc[(df_adult['sex'] == sex_val) & (df_adult['income'] == income_val), 'sample_weight'] = weight\n",
    "\n",
    "    print(\"\\n--- Distribution of Income by Gender After Reweighing (Weighted Counts) ---\")\n",
    "    # Calculate weighted counts\n",
    "    weighted_distribution = df_adult.groupby(['sex', 'income'])['sample_weight'].sum().unstack(fill_value=0)\n",
    "    print(\"Weighted counts:\")\n",
    "    print(weighted_distribution)\n",
    "\n",
    "    # Calculate weighted percentages\n",
    "    # Sum of weights for each gender group\n",
    "    total_weight_female = weighted_distribution.loc['Female'].sum()\n",
    "    total_weight_male = weighted_distribution.loc['Male'].sum()\n",
    "\n",
    "    # Calculate weighted percentages within each gender group\n",
    "    weighted_percentage_female = (weighted_distribution.loc['Female'] / total_weight_female) * 100\n",
    "    weighted_percentage_male = (weighted_distribution.loc['Male'] / total_weight_male) * 100\n",
    "\n",
    "    weighted_percentage_df = pd.DataFrame([weighted_percentage_female, weighted_percentage_male], index=['Female', 'Male'])\n",
    "    print(\"\\nRow-wise weighted percentages (Income distribution within each Gender after reweighing):\")\n",
    "    print(weighted_percentage_df.round(2))\n",
    "\n",
    "    print(\"\\n--- Visualizing the Impact of Reweighing ---\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "    # Plot original distribution\n",
    "    sns.countplot(x='sex', hue='income', data=df_adult, ax=axes[0], palette='coolwarm')\n",
    "    axes[0].set_title('Original Income Distribution by Gender', fontsize=14)\n",
    "    axes[0].set_xlabel('Gender', fontsize=12)\n",
    "    axes[0].set_ylabel('Number of Samples', fontsize=12)\n",
    "    axes[0].legend(title='Income')\n",
    "    axes[0].grid(axis='y', linestyle=':', alpha=0.7)\n",
    "\n",
    "\n",
    "    # Plot weighted distribution (using the calculated weights for visualization)\n",
    "    # For visualization, we can use a bar plot of the weighted counts\n",
    "    weighted_distribution.plot(kind='bar', ax=axes[1], color=['#66c2a5', '#fc8d62']) # Using different colors\n",
    "    axes[1].set_title('Weighted Income Distribution by Gender', fontsize=14)\n",
    "    axes[1].set_xlabel('Gender', fontsize=12)\n",
    "    axes[1].set_ylabel('Weighted Number of Samples', fontsize=12)\n",
    "    axes[1].legend(title='Income')\n",
    "    axes[1].tick_params(axis='x', rotation=0) # Ensure x-axis labels are not rotated\n",
    "    axes[1].grid(axis='y', linestyle=':', alpha=0.7)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nReweighing has been applied. The weighted distributions should show a more balanced representation of income levels across different genders compared to the original distribution.\")\n",
    "    print(\"This technique aims to reduce bias by giving more importance to underrepresented groups during model training.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing the Adult Income dataset for bias mitigation: {e}\")\n",
    "    print(\"Please ensure the dataset URL is correct and accessible, and that necessary libraries are installed.\")\n",
    "\n",
    "\n",
    "# --- New Section: Ensuring Consistency Across Training & Inference Datasets: Pipeline Integration ---\n",
    "print(\"\\n\\n--- Ensuring Consistency Across Training & Inference Datasets: Pipeline Integration ---\")\n",
    "\n",
    "print(\"Generating a synthetic dataset for demonstrating pipeline integration...\")\n",
    "\n",
    "# Generate a synthetic dataset for classification\n",
    "np.random.seed(0) # for reproducibility\n",
    "n_samples = 300\n",
    "X_pipeline = pd.DataFrame({\n",
    "    'Feature_A': np.random.normal(loc=100, scale=20, size=n_samples),\n",
    "    'Feature_B': np.random.normal(loc=5, scale=1.5, size=n_samples)\n",
    "})\n",
    "# Create a binary target variable\n",
    "y_pipeline = (X_pipeline['Feature_A'] + X_pipeline['Feature_B'] * 10 > 150).astype(int)\n",
    "\n",
    "print(\"\\nOriginal synthetic dataset for pipeline (first 5 rows):\")\n",
    "print(X_pipeline.head())\n",
    "print(\"\\nOriginal synthetic dataset target distribution:\")\n",
    "print(y_pipeline.value_counts())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_pipeline, X_test_pipeline, y_train_pipeline, y_test_pipeline = train_test_split(\n",
    "    X_pipeline, y_pipeline, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining data shape for pipeline: {X_train_pipeline.shape}\")\n",
    "print(f\"Testing data shape for pipeline: {X_test_pipeline.shape}\")\n",
    "\n",
    "print(\"\\n--- Creating and Training a Scikit-learn Pipeline ---\")\n",
    "\n",
    "# Define the pipeline steps:\n",
    "# 1. StandardScaler: To scale the features\n",
    "# 2. LogisticRegression: A simple classification model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),        # Step 1: Feature Scaling\n",
    "    ('classifier', LogisticRegression()) # Step 2: Classification Model\n",
    "])\n",
    "\n",
    "print(\"\\nPipeline created with steps:\")\n",
    "print(pipeline)\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "# When .fit() is called on the pipeline, it sequentially calls fit_transform()\n",
    "# on all transformers (like StandardScaler) and then fit() on the final estimator.\n",
    "print(\"\\nTraining the pipeline...\")\n",
    "pipeline.fit(X_train_pipeline, y_train_pipeline)\n",
    "print(\"Pipeline training complete.\")\n",
    "\n",
    "print(\"\\n--- Demonstrating Inference with the Trained Pipeline ---\")\n",
    "\n",
    "# Make predictions on the test data using the trained pipeline\n",
    "# When .predict() is called on the pipeline, it sequentially calls transform()\n",
    "# on all transformers and then predict() on the final estimator.\n",
    "# This ensures that the same scaling parameters learned from the training data\n",
    "# are applied to the test data automatically.\n",
    "y_pred_pipeline = pipeline.predict(X_test_pipeline)\n",
    "\n",
    "print(\"\\nFirst 10 actual labels from test set:\", y_test_pipeline.head(10).tolist())\n",
    "print(\"First 10 predicted labels from test set:\", y_pred_pipeline[:10].tolist())\n",
    "\n",
    "# Evaluate the pipeline's performance (optional, but good practice)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test_pipeline, y_pred_pipeline)\n",
    "print(f\"\\nAccuracy of the pipeline on the test set: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nPipeline integration demonstration complete.\")\n",
    "print(\"Using a scikit-learn Pipeline ensures that all preprocessing steps (like scaling) are consistently applied to both training and new inference data.\")\n",
    "print(\"This prevents data leakage and ensures that your model receives data in the exact same format it was trained on.\")\n",
    "\n",
    "# --- New Section: Feature Engineering Best Practices: Handling Text Data ---\n",
    "print(\"\\n\\n--- Feature Engineering Best Practices: Handling Text Data ---\")\n",
    "\n",
    "# Define the URL for the SMS Spam Collection dataset\n",
    "# This dataset contains SMS messages labeled as 'ham' (legitimate) or 'spam'.\n",
    "sms_spam_url = \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/spam/smsspamcollection.zip\"\n",
    "\n",
    "print(\"Loading the SMS Spam Collection dataset...\")\n",
    "try:\n",
    "    # The dataset is a zip file containing a tab-separated text file.\n",
    "    # We need to read it directly from the zip.\n",
    "    df_sms = pd.read_csv(sms_spam_url, sep='\\t', header=None, names=['label', 'message'], encoding='latin-1')\n",
    "    print(\"SMS Spam Collection dataset loaded successfully.\")\n",
    "    print(\"\\nFirst 5 rows of the SMS dataset:\")\n",
    "    print(df_sms.head())\n",
    "\n",
    "    print(\"\\n--- Initial Class Distribution (Spam vs. Ham) ---\")\n",
    "    print(df_sms['label'].value_counts())\n",
    "\n",
    "    print(\"\\n--- Text Preprocessing ---\")\n",
    "\n",
    "    # Initialize stemmer and stopwords\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        # 1. Lowercase the text\n",
    "        text = text.lower()\n",
    "        # 2. Remove punctuation and numbers\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        # 3. Tokenize and remove stop words, then stem\n",
    "        words = text.split()\n",
    "        words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    # Apply preprocessing to the 'message' column\n",
    "    print(\"Applying text preprocessing (lowercasing, removing punctuation/numbers, stop words, stemming)...\")\n",
    "    df_sms['cleaned_message'] = df_sms['message'].apply(preprocess_text)\n",
    "    print(\"\\nOriginal message vs. Cleaned message (first 5 examples):\")\n",
    "    for i in range(5):\n",
    "        print(f\"Original: {df_sms['message'].iloc[i]}\")\n",
    "        print(f\"Cleaned:  {df_sms['cleaned_message'].iloc[i]}\\n\")\n",
    "\n",
    "    print(\"\\n--- Feature Extraction using TF-IDF ---\")\n",
    "\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    # max_features limits the number of features (vocabulary size)\n",
    "    # min_df ignores terms that appear in too few documents\n",
    "    # max_df ignores terms that appear in too many documents\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.8)\n",
    "\n",
    "    # Fit the vectorizer on the cleaned messages and transform them\n",
    "    # This converts text data into numerical TF-IDF features\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df_sms['cleaned_message'])\n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame for easier inspection (optional, for small datasets)\n",
    "    # For very large datasets, keep it as a sparse matrix for memory efficiency\n",
    "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    print(f\"\\nShape of TF-IDF features: {X_tfidf.shape}\")\n",
    "    print(\"\\nFirst 5 rows of TF-IDF features (showing a subset of columns):\")\n",
    "    print(X_tfidf_df.iloc[:5, :10]) # Display first 5 rows and first 10 columns\n",
    "\n",
    "    print(\"\\nTF-IDF feature extraction complete.\")\n",
    "    print(\"These numerical features can now be used to train machine learning models for tasks like spam detection.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing the SMS Spam Collection dataset: {e}\")\n",
    "    print(\"Please ensure the dataset URL is correct and accessible, and that NLTK data (stopwords, punkt) is downloaded.\")\n",
    "\n",
    "\n",
    "# --- New Section: Data Drift: Detection Using Statistical Tests (Kolmogorov-Smirnov) ---\n",
    "print(\"\\n\\n--- Data Drift: Detection Using Statistical Tests (Kolmogorov-Smirnov) ---\")\n",
    "\n",
    "print(\"Simulating two datasets with a distribution shift for drift detection...\")\n",
    "\n",
    "# Simulate Dataset 1 (Reference/Baseline Data)\n",
    "# A normal distribution with mean 0 and standard deviation 1\n",
    "np.random.seed(10) # for reproducibility\n",
    "dataset1 = np.random.normal(loc=0, scale=1, size=500)\n",
    "\n",
    "# Simulate Dataset 2 (New/Current Data with a shift)\n",
    "# A normal distribution with a shifted mean and slightly different standard deviation\n",
    "dataset2 = np.random.normal(loc=0.5, scale=1.2, size=500)\n",
    "\n",
    "print(f\"\\nDataset 1 (Reference) - Mean: {np.mean(dataset1):.2f}, Std Dev: {np.std(dataset1):.2f}\")\n",
    "print(f\"Dataset 2 (Current) - Mean: {np.mean(dataset2):.2f}, Std Dev: {np.std(dataset2):.2f}\")\n",
    "\n",
    "# --- Visualize the Distributions ---\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(dataset1, color='blue', label='Dataset 1 (Reference)', kde=True, stat='density', alpha=0.6)\n",
    "sns.histplot(dataset2, color='red', label='Dataset 2 (Current)', kde=True, stat='density', alpha=0.6)\n",
    "plt.title('Distribution of Simulated Datasets', fontsize=14)\n",
    "plt.xlabel('Value', fontsize=12)\n",
    "plt.ylabel('Density', fontsize=12)\n",
    "plt.legend()\n",
    "plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Applying Kolmogorov-Smirnov (K-S) Test ---\")\n",
    "# The Kolmogorov-Smirnov test is a non-parametric test that compares the cumulative\n",
    "# distribution functions (CDFs) of two samples.\n",
    "# H0 (Null Hypothesis): The two samples are drawn from the same continuous distribution.\n",
    "# H1 (Alternative Hypothesis): The two samples are drawn from different distributions.\n",
    "\n",
    "# Perform the K-S test\n",
    "statistic, p_value = kstest(dataset1, dataset2)\n",
    "\n",
    "print(f\"K-S Test Statistic: {statistic:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "print(f\"\\nSignificance Level (alpha): {alpha}\")\n",
    "\n",
    "# Interpret the results\n",
    "if p_value < alpha:\n",
    "    print(f\"Since p-value ({p_value:.4f}) < alpha ({alpha}), we reject the null hypothesis.\")\n",
    "    print(\"Conclusion: There is a statistically significant difference between the distributions of Dataset 1 and Dataset 2.\")\n",
    "    print(\"This indicates that data drift has likely occurred.\")\n",
    "else:\n",
    "    print(f\"Since p-value ({p_value:.4f}) >= alpha ({alpha}), we fail to reject the null hypothesis.\")\n",
    "    print(\"Conclusion: There is no statistically significant difference between the distributions of Dataset 1 and Dataset 2.\")\n",
    "    print(\"This suggests that data drift has not occurred (or is not detectable at this significance level).\")\n",
    "\n",
    "print(\"\\nData drift detection using the Kolmogorov-Smirnov test complete.\")\n",
    "print(\"This statistical test helps quantify the difference between two data distributions, indicating potential data drift.\")\n",
    "\n",
    "\n",
    "# --- New Section: Implementing Adversarial Validation for Data Drift ---\n",
    "print(\"\\n\\n--- Implementing Adversarial Validation for Data Drift ---\")\n",
    "\n",
    "print(\"Generating two synthetic datasets (train and test) with a subtle difference for adversarial validation...\")\n",
    "\n",
    "# Simulate 'training' dataset\n",
    "np.random.seed(1) # for reproducibility\n",
    "X_train_adv = pd.DataFrame({\n",
    "    'feature_A': np.random.normal(loc=10, scale=2, size=500),\n",
    "    'feature_B': np.random.normal(loc=50, scale=5, size=500)\n",
    "})\n",
    "\n",
    "# Simulate 'test' dataset with a slight shift in distribution for one feature\n",
    "np.random.seed(2) # different seed for test data\n",
    "X_test_adv = pd.DataFrame({\n",
    "    'feature_A': np.random.normal(loc=10.5, scale=2, size=500), # Slightly shifted mean\n",
    "    'feature_B': np.random.normal(loc=50, scale=5, size=500)\n",
    "})\n",
    "\n",
    "print(\"\\nFirst 5 rows of simulated training data:\")\n",
    "print(X_train_adv.head())\n",
    "print(\"\\nFirst 5 rows of simulated test data:\")\n",
    "print(X_test_adv.head())\n",
    "\n",
    "print(\"\\n--- Preparing Data for Adversarial Classifier ---\")\n",
    "\n",
    "# Create a target variable indicating the origin of the data\n",
    "# 0 for training data, 1 for test data\n",
    "y_train_adv = pd.Series(0, index=X_train_adv.index)\n",
    "y_test_adv = pd.Series(1, index=X_test_adv.index)\n",
    "\n",
    "# Concatenate the datasets\n",
    "X_combined = pd.concat([X_train_adv, X_test_adv], ignore_index=True)\n",
    "y_combined = pd.concat([y_train_adv, y_test_adv], ignore_index=True)\n",
    "\n",
    "# Split the combined dataset for training the adversarial classifier\n",
    "# This split is for the adversarial validation process itself, not the original model training.\n",
    "X_adv_train, X_adv_test, y_adv_train, y_adv_test = train_test_split(\n",
    "    X_combined, y_combined, test_size=0.3, random_state=42, stratify=y_combined\n",
    ")\n",
    "\n",
    "print(f\"\\nCombined data shape: {X_combined.shape}\")\n",
    "print(f\"Adversarial classifier training data shape: {X_adv_train.shape}\")\n",
    "print(f\"Adversarial classifier test data shape: {X_adv_test.shape}\")\n",
    "print(f\"Adversarial classifier target distribution in training set:\\n{y_adv_train.value_counts()}\")\n",
    "\n",
    "\n",
    "print(\"\\n--- Training Adversarial Classifier ---\")\n",
    "\n",
    "# Create a pipeline for the adversarial classifier: scale features and then use Logistic Regression\n",
    "# A simple classifier is usually sufficient for adversarial validation.\n",
    "adversarial_pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "print(\"\\nAdversarial pipeline created:\")\n",
    "print(adversarial_pipeline)\n",
    "\n",
    "# Train the adversarial classifier\n",
    "print(\"\\nTraining the adversarial classifier...\")\n",
    "adversarial_pipeline.fit(X_adv_train, y_adv_train)\n",
    "print(\"Adversarial classifier training complete.\")\n",
    "\n",
    "print(\"\\n--- Evaluating Adversarial Classifier Performance ---\")\n",
    "\n",
    "# Predict probabilities on the adversarial test set\n",
    "y_adv_pred_proba = adversarial_pipeline.predict_proba(X_adv_test)[:, 1]\n",
    "y_adv_pred = adversarial_pipeline.predict(X_adv_test)\n",
    "\n",
    "# Calculate AUC-ROC score\n",
    "auc_score = roc_auc_score(y_adv_test, y_adv_pred_proba)\n",
    "accuracy = accuracy_score(y_adv_test, y_adv_pred)\n",
    "\n",
    "print(f\"\\nAdversarial Classifier Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Adversarial Classifier AUC-ROC Score: {auc_score:.4f}\")\n",
    "\n",
    "print(\"\\n--- Interpreting Adversarial Validation Results ---\")\n",
    "print(\"If the AUC-ROC score is significantly higher than 0.5 (e.g., closer to 1.0),\")\n",
    "print(\"it means the classifier can effectively distinguish between the training and test datasets.\")\n",
    "print(\"A high AUC-ROC score indicates that there is a detectable data drift between the datasets.\")\n",
    "print(\"Conversely, an AUC-ROC score close to 0.5 suggests no significant drift, as the classifier performs no better than random guessing.\")\n",
    "\n",
    "if auc_score > 0.7: # A common heuristic for indicating significant drift\n",
    "    print(f\"\\nConclusion: With an AUC-ROC score of {auc_score:.4f}, there is strong evidence of data drift.\")\n",
    "    print(\"This suggests that the distribution of features in the 'test' dataset is different from the 'training' dataset.\")\n",
    "else:\n",
    "    print(f\"Since an AUC-ROC score of {auc_score:.4f} is not significantly higher than 0.5, there is no strong evidence of data drift.\")\n",
    "    print(\"The distributions of features in the 'test' and 'training' datasets appear similar.\")\n",
    "\n",
    "print(\"\\nAdversarial validation for data drift detection complete.\")\n",
    "print(\"This technique provides a powerful way to detect subtle shifts in data distributions that might impact model performance.\")\n",
    "\n",
    "\n",
    "# --- New Section: Using SHAP for Feature Drift Analysis ---\n",
    "print(\"\\n\\n--- Using SHAP for Feature Drift Analysis ---\")\n",
    "\n",
    "print(\"Simulating two datasets with a feature drift for SHAP analysis...\")\n",
    "\n",
    "# Simulate Dataset 1 (Time Period 1)\n",
    "np.random.seed(100) # for reproducibility\n",
    "n_samples_shap = 500\n",
    "X_time1 = pd.DataFrame({\n",
    "    'Feature_A': np.random.normal(loc=10, scale=2, size=n_samples_shap),\n",
    "    'Feature_B': np.random.normal(loc=50, scale=5, size=n_samples_shap),\n",
    "    'Feature_C': np.random.normal(loc=100, scale=10, size=n_samples_shap)\n",
    "})\n",
    "# Target variable for Time 1: Feature_A is highly influential\n",
    "y_time1 = ((X_time1['Feature_A'] * 0.8 + X_time1['Feature_B'] * 0.1 + X_time1['Feature_C'] * 0.05) > 10).astype(int)\n",
    "\n",
    "# Simulate Dataset 2 (Time Period 2) - Introduce feature drift in Feature_C's influence\n",
    "np.random.seed(101) # different seed for time 2\n",
    "X_time2 = pd.DataFrame({\n",
    "    'Feature_A': np.random.normal(loc=10, scale=2, size=n_samples_shap),\n",
    "    'Feature_B': np.random.normal(loc=50, scale=5, size=n_samples_shap),\n",
    "    'Feature_C': np.random.normal(loc=105, scale=12, size=n_samples_shap) # Slight distribution shift\n",
    "})\n",
    "# Target variable for Time 2: Feature_C's influence has changed (e.g., increased)\n",
    "y_time2 = ((X_time2['Feature_A'] * 0.7 + X_time2['Feature_B'] * 0.1 + X_time2['Feature_C'] * 0.2) > 10).astype(int) # Increased coefficient for Feature_C\n",
    "\n",
    "\n",
    "print(\"\\nDataset 1 (Time Period 1) - First 5 rows:\")\n",
    "print(X_time1.head())\n",
    "print(\"\\nDataset 2 (Time Period 2) - First 5 rows:\")\n",
    "print(X_time2.head())\n",
    "\n",
    "print(\"\\n--- Training Models for Each Time Period ---\")\n",
    "\n",
    "# Train a RandomForestClassifier on Dataset 1\n",
    "model_time1 = RandomForestClassifier(random_state=42, n_estimators=50)\n",
    "model_time1.fit(X_time1, y_time1)\n",
    "print(\"Model trained on Dataset 1 (Time Period 1).\")\n",
    "\n",
    "# Train a RandomForestClassifier on Dataset 2\n",
    "model_time2 = RandomForestClassifier(random_state=42, n_estimators=50)\n",
    "model_time2.fit(X_time2, y_time2)\n",
    "print(\"Model trained on Dataset 2 (Time Period 2).\")\n",
    "\n",
    "print(\"\\n--- Calculating SHAP Values for Each Model ---\")\n",
    "\n",
    "# Create SHAP explainers for each model\n",
    "# For tree-based models, shap.TreeExplainer is efficient\n",
    "explainer_time1 = shap.TreeExplainer(model_time1)\n",
    "explainer_time2 = shap.TreeExplainer(model_time2)\n",
    "\n",
    "# Calculate SHAP values for each dataset\n",
    "# We'll use a subset of the data for explanation for faster computation, or the full dataset if small.\n",
    "# For demonstration, let's use the full datasets.\n",
    "shap_values_time1 = explainer_time1.shap_values(X_time1)\n",
    "shap_values_time2 = explainer_time2.shap_values(X_time2)\n",
    "\n",
    "# SHAP values for binary classification models often return two arrays (for class 0 and class 1).\n",
    "# We are interested in the impact on the positive class (class 1).\n",
    "if isinstance(shap_values_time1, list):\n",
    "    shap_values_time1 = shap_values_time1[1] # Take SHAP values for the positive class\n",
    "if isinstance(shap_values_time2, list):\n",
    "    shap_values_time2 = shap_values_time2[1] # Take SHAP values for the positive class\n",
    "\n",
    "print(f\"SHAP values calculated for Dataset 1 (shape: {shap_values_time1.shape}).\")\n",
    "print(f\"SHAP values calculated for Dataset 2 (shape: {shap_values_time2.shape}).\")\n",
    "\n",
    "print(\"\\n--- Analyzing Feature Importance Changes (Feature Drift) using SHAP ---\")\n",
    "\n",
    "# Calculate mean absolute SHAP values for each feature in each time period\n",
    "# This represents the average magnitude of impact each feature has on the model's output.\n",
    "mean_abs_shap_time1 = np.abs(shap_values_time1).mean(axis=0)\n",
    "mean_abs_shap_time2 = np.abs(shap_values_time2).mean(axis=0)\n",
    "\n",
    "# Create a DataFrame to compare feature importances\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': X_time1.columns,\n",
    "    'SHAP_Importance_Time1': mean_abs_shap_time1,\n",
    "    'SHAP_Importance_Time2': mean_abs_shap_time2\n",
    "})\n",
    "feature_importance_df['Change'] = feature_importance_df['SHAP_Importance_Time2'] - feature_importance_df['SHAP_Importance_Time1']\n",
    "feature_importance_df['Percentage_Change'] = (feature_importance_df['Change'] / feature_importance_df['SHAP_Importance_Time1']) * 100\n",
    "feature_importance_df = feature_importance_df.sort_values(by='SHAP_Importance_Time1', ascending=False)\n",
    "\n",
    "print(\"\\nComparison of Feature Importances (Mean Absolute SHAP Values):\")\n",
    "print(feature_importance_df.round(4))\n",
    "\n",
    "# --- Visualize Feature Importance Changes ---\n",
    "plt.figure(figsize=(12, 7))\n",
    "bar_width = 0.35\n",
    "index = np.arange(len(feature_importance_df['Feature']))\n",
    "\n",
    "plt.bar(index, feature_importance_df['SHAP_Importance_Time1'], bar_width, label='Time Period 1', color='skyblue')\n",
    "plt.bar(index + bar_width, feature_importance_df['SHAP_Importance_Time2'], bar_width, label='Time Period 2', color='lightcoral')\n",
    "\n",
    "plt.xlabel('Feature', fontsize=12)\n",
    "plt.ylabel('Mean Absolute SHAP Value', fontsize=12)\n",
    "plt.title('Feature Importance Comparison Across Time Periods (Indicating Feature Drift)', fontsize=16)\n",
    "plt.xticks(index + bar_width / 2, feature_importance_df['Feature'], rotation=45, ha='right', fontsize=10)\n",
    "plt.legend(fontsize=10)\n",
    "plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nSHAP-based feature drift analysis complete.\")\n",
    "print(\"The comparison of mean absolute SHAP values and the visualization highlight how the importance of 'Feature_C' has changed between Time Period 1 and Time Period 2, indicating feature drift.\")\n",
    "print(\"This technique is valuable for understanding which features are contributing differently to model predictions over time, signaling a change in data characteristics.\")\n",
    "\n",
    "\n",
    "# --- New Section: Detect Schema Mismatches in Data Pipelines ---\n",
    "print(\"\\n\\n--- Detect Schema Mismatches in Data Pipelines ---\")\n",
    "\n",
    "print(\"Simulating source and target DataFrames with potential schema mismatches...\")\n",
    "\n",
    "# 1. Load the source DataFrame with the specified schema\n",
    "source_data = {\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'age': [24, 30, 22, 35]\n",
    "}\n",
    "source_df = pd.DataFrame(source_data)\n",
    "print(\"\\nSource DataFrame:\")\n",
    "print(source_df)\n",
    "\n",
    "# 2. Load the target DataFrame with the specified schema\n",
    "target_data = {\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'fullname': ['Alice Smith', 'Bob Johnson', 'Charlie Brown', 'David Lee'],\n",
    "    'age': [24, 30, 22, 35]\n",
    "}\n",
    "target_df = pd.DataFrame(target_data)\n",
    "print(\"\\nTarget DataFrame (before resolving mismatch):\")\n",
    "print(target_df)\n",
    "\n",
    "# 3. Use a simple function to detect mismatches in column names\n",
    "def detect_column_mismatch(df1, df2):\n",
    "    columns1 = set(df1.columns)\n",
    "    columns2 = set(df2.columns)\n",
    "\n",
    "    mismatched_in_df1 = list(columns1 - columns2)\n",
    "    mismatched_in_df2 = list(columns2 - columns1)\n",
    "    common_columns = list(columns1.intersection(columns2))\n",
    "\n",
    "    if mismatched_in_df1 or mismatched_in_df2:\n",
    "        print(\"\\n--- Schema Mismatch Detected! ---\")\n",
    "        if mismatched_in_df1:\n",
    "            print(f\"Columns in Source but not in Target: {mismatched_in_df1}\")\n",
    "        if mismatched_in_df2:\n",
    "            print(f\"Columns in Target but not in Source: {mismatched_in_df2}\")\n",
    "        print(f\"Common columns: {common_columns}\")\n",
    "        return True\n",
    "    else:\n",
    "        print(\"\\nNo column name mismatches detected.\")\n",
    "        return False\n",
    "\n",
    "# Detect mismatch\n",
    "mismatch_found = detect_column_mismatch(source_df, target_df)\n",
    "\n",
    "# 4. Resolve the mismatch by renaming the `fullname` column in the target DataFrame to `name`\n",
    "if mismatch_found:\n",
    "    print(\"\\n--- Resolving Mismatch: Renaming 'fullname' to 'name' in Target DataFrame ---\")\n",
    "    if 'fullname' in target_df.columns:\n",
    "        target_df.rename(columns={'fullname': 'name'}, inplace=True)\n",
    "        print(\"Column 'fullname' in Target DataFrame renamed to 'name'.\")\n",
    "    else:\n",
    "        print(\"Column 'fullname' not found in Target DataFrame to rename.\")\n",
    "\n",
    "    print(\"\\nTarget DataFrame (after resolving mismatch):\")\n",
    "    print(target_df)\n",
    "\n",
    "    # Verify after resolution\n",
    "    print(\"\\n--- Verifying Schema After Resolution ---\")\n",
    "    detect_column_mismatch(source_df, target_df)\n",
    "else:\n",
    "    print(\"\\nNo mismatch to resolve.\")\n",
    "\n",
    "print(\"\\nSchema mismatch detection and resolution complete. This helps ensure data consistency across pipelines.\")\n",
    "\n",
    "\n",
    "# --- New Section: Detect Data Drift in ML Models: Categorical Feature Drift ---\n",
    "print(\"\\n\\n--- Detect Data Drift in ML Models: Categorical Feature Drift ---\")\n",
    "\n",
    "print(\"Simulating baseline and current datasets with a categorical feature for drift detection...\")\n",
    "\n",
    "# 1. Load the baseline distribution for a categorical feature (e.g., gender)\n",
    "# Simulate a baseline distribution for 'gender' where 'Male' is more frequent\n",
    "np.random.seed(20) # for reproducibility\n",
    "baseline_gender_data = np.random.choice(['Male', 'Female', 'Other'], size=1000, p=[0.6, 0.35, 0.05])\n",
    "df_baseline = pd.DataFrame({'gender': baseline_gender_data})\n",
    "\n",
    "print(\"\\nBaseline Gender Distribution:\")\n",
    "print(df_baseline['gender'].value_counts(normalize=True).round(2))\n",
    "\n",
    "# 2. Load the same feature from your current production data\n",
    "# Simulate a current distribution for 'gender' where 'Female' frequency has increased\n",
    "np.random.seed(21) # different seed for current data\n",
    "current_gender_data = np.random.choice(['Male', 'Female', 'Other'], size=1000, p=[0.45, 0.5, 0.05])\n",
    "df_current = pd.DataFrame({'gender': current_gender_data})\n",
    "\n",
    "print(\"\\nCurrent Gender Distribution:\")\n",
    "print(df_current['gender'].value_counts(normalize=True).round(2))\n",
    "\n",
    "print(\"\\n--- Visualizing Categorical Feature Distributions ---\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n",
    "\n",
    "sns.countplot(x='gender', data=df_baseline, ax=axes[0], palette='pastel')\n",
    "axes[0].set_title('Baseline Gender Distribution', fontsize=14)\n",
    "axes[0].set_xlabel('Gender', fontsize=12)\n",
    "axes[0].set_ylabel('Count', fontsize=12)\n",
    "axes[0].grid(axis='y', linestyle=':', alpha=0.7)\n",
    "\n",
    "sns.countplot(x='gender', data=df_current, ax=axes[1], palette='pastel')\n",
    "axes[1].set_title('Current Gender Distribution', fontsize=14)\n",
    "axes[1].set_xlabel('Gender', fontsize=12)\n",
    "axes[1].set_ylabel('Count', fontsize=12)\n",
    "axes[1].grid(axis='y', linestyle=':', alpha=0.7)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Using Chi-squared Test to Compare Distributions ---\")\n",
    "# Create a contingency table (observed frequencies)\n",
    "# This table shows the counts of each gender category in both baseline and current data.\n",
    "contingency_table = pd.DataFrame({\n",
    "    'Baseline': df_baseline['gender'].value_counts().reindex(['Male', 'Female', 'Other'], fill_value=0),\n",
    "    'Current': df_current['gender'].value_counts().reindex(['Male', 'Female', 'Other'], fill_value=0)\n",
    "})\n",
    "\n",
    "print(\"\\nContingency Table:\")\n",
    "print(contingency_table)\n",
    "\n",
    "# Perform the Chi-squared test\n",
    "# H0 (Null Hypothesis): The two distributions are independent (i.e., no significant drift).\n",
    "# H1 (Alternative Hypothesis): The two distributions are dependent (i.e., significant drift).\n",
    "chi2_stat, p_value, dof, expected = chi2_contingency(contingency_table)\n",
    "\n",
    "print(f\"\\nChi-squared Statistic: {chi2_stat:.4f}\")\n",
    "print(f\"P-value: {p_value:.4f}\")\n",
    "print(f\"Degrees of Freedom: {dof}\")\n",
    "# print(\"\\nExpected Frequencies (if no drift):\")\n",
    "# print(pd.DataFrame(expected, index=contingency_table.index, columns=contingency_table.columns).round(2))\n",
    "\n",
    "# Set a significance level (alpha)\n",
    "alpha = 0.05\n",
    "print(f\"\\nSignificance Level (alpha): {alpha}\")\n",
    "\n",
    "# Interpret the results\n",
    "if p_value < alpha:\n",
    "    print(f\"Since p-value ({p_value:.4f}) < alpha ({alpha}), we reject the null hypothesis.\")\n",
    "    print(\"Conclusion: There is a statistically significant difference between the baseline and current categorical feature distributions.\")\n",
    "    print(\"This indicates that data drift has likely occurred in the 'gender' feature.\")\n",
    "else:\n",
    "    print(f\"Since p-value ({p_value:.4f}) >= alpha ({alpha}), we fail to reject the null hypothesis.\")\n",
    "    print(\"Conclusion: There is no statistically significant difference between the baseline and current categorical feature distributions.\")\n",
    "    print(\"This suggests that data drift has not occurred in the 'gender' feature (or is not detectable at this significance level).\")\n",
    "\n",
    "print(\"\\nCategorical feature drift detection using the Chi-squared test complete.\")\n",
    "print(\"This method helps identify shifts in the distribution of categorical variables over time.\")\n",
    "\n",
    "\n",
    "# --- New Section: Detect Data Drift in ML Models: Feature Correlation Drift ---\n",
    "print(\"\\n\\n--- Detect Data Drift in ML Models: Feature Correlation Drift ---\")\n",
    "\n",
    "print(\"Simulating baseline and current datasets with changing feature correlations...\")\n",
    "\n",
    "# 1. Simulate the baseline dataset (training data)\n",
    "np.random.seed(30) # for reproducibility\n",
    "n_samples_corr = 500\n",
    "baseline_data_corr = pd.DataFrame({\n",
    "    'Feature_X': np.random.normal(loc=0, scale=1, size=n_samples_corr),\n",
    "    'Feature_Y': np.random.normal(loc=0, scale=1, size=n_samples_corr),\n",
    "    'Feature_Z': np.random.normal(loc=0, scale=1, size=n_samples_corr)\n",
    "})\n",
    "# Introduce a strong positive correlation between X and Y in baseline\n",
    "baseline_data_corr['Feature_Y'] = baseline_data_corr['Feature_X'] * 0.7 + np.random.normal(loc=0, scale=0.5, size=n_samples_corr)\n",
    "\n",
    "# 2. Simulate the current dataset (production data)\n",
    "np.random.seed(31) # different seed for current data\n",
    "current_data_corr = pd.DataFrame({\n",
    "    'Feature_X': np.random.normal(loc=0, scale=1, size=n_samples_corr),\n",
    "    'Feature_Y': np.random.normal(loc=0, scale=1, size=n_samples_corr),\n",
    "    'Feature_Z': np.random.normal(loc=0, scale=1, size=n_samples_corr)\n",
    "})\n",
    "# Introduce a weaker or negative correlation between X and Y in current data\n",
    "current_data_corr['Feature_Y'] = baseline_data_corr['Feature_X'] * 0.2 + np.random.normal(loc=0, scale=0.8, size=n_samples_corr)\n",
    "\n",
    "\n",
    "print(\"\\nBaseline Data (first 5 rows):\")\n",
    "print(baseline_data_corr.head())\n",
    "print(\"\\nCurrent Data (first 5 rows):\")\n",
    "print(current_data_corr.head())\n",
    "\n",
    "print(\"\\n--- Computing Correlation Matrices ---\")\n",
    "\n",
    "# Compute correlation matrix for the baseline dataset\n",
    "correlation_matrix_baseline = baseline_data_corr.corr()\n",
    "print(\"\\nCorrelation Matrix - Baseline Data:\")\n",
    "print(correlation_matrix_baseline.round(2))\n",
    "\n",
    "# Compute correlation matrix for the current dataset\n",
    "correlation_matrix_current = current_data_corr.corr()\n",
    "print(\"\\nCorrelation Matrix - Current Data:\")\n",
    "print(correlation_matrix_current.round(2))\n",
    "\n",
    "print(\"\\n--- Assessing Changes in Correlation Matrix ---\")\n",
    "\n",
    "# Calculate the absolute difference between the two correlation matrices\n",
    "correlation_difference = np.abs(correlation_matrix_baseline - correlation_matrix_current)\n",
    "print(\"\\nAbsolute Difference in Correlation Matrices (Current - Baseline):\")\n",
    "print(correlation_difference.round(2))\n",
    "\n",
    "# Visualize the correlation matrices as heatmaps\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "sns.heatmap(correlation_matrix_baseline, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, ax=axes[0])\n",
    "axes[0].set_title('Baseline Correlation Matrix', fontsize=14)\n",
    "\n",
    "sns.heatmap(correlation_matrix_current, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5, ax=axes[1])\n",
    "axes[1].set_title('Current Correlation Matrix', fontsize=14)\n",
    "\n",
    "sns.heatmap(correlation_difference, annot=True, cmap='Reds', fmt=\".2f\", linewidths=.5, ax=axes[2])\n",
    "axes[2].set_title('Absolute Difference in Correlations', fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n--- Interpreting Feature Correlation Drift ---\")\n",
    "print(\"Significant changes in the correlation matrix, especially large values in the 'Absolute Difference' heatmap, indicate feature correlation drift.\")\n",
    "print(\"For example, observe the correlation between 'Feature_X' and 'Feature_Y'. In the baseline, it was around 0.70, but in the current data, it dropped to around 0.20.\")\n",
    "print(\"This change is clearly visible in the 'Absolute Difference' matrix, showing a large value (around 0.50) for this pair.\")\n",
    "print(\"Such shifts can impact models that rely on feature relationships (e.g., linear models, PCA).\")\n",
    "print(\"Investigating these changes is crucial to understand if there are issues in data collection, new underlying patterns, or if the model assumptions are no longer valid.\")\n",
    "\n",
    "print(\"\\nFeature correlation drift detection complete.\")\n",
    "\n",
    "\n",
    "# --- New Section: Task 1: Handling Missing Values - Simple Imputation ---\n",
    "print(\"\\n\\n--- Task 1: Handling Missing Values - Simple Imputation ---\")\n",
    "\n",
    "print(\"Simulating a DataFrame with missing values for imputation demonstration...\")\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "np.random.seed(40)\n",
    "data_imputation = {\n",
    "    'numerical_feature_1': np.random.normal(loc=100, scale=10, size=20),\n",
    "    'numerical_feature_2': np.random.randint(1, 100, size=20).astype(float),\n",
    "    'categorical_feature': np.random.choice(['A', 'B', 'C'], size=20)\n",
    "}\n",
    "df_imputation = pd.DataFrame(data_imputation)\n",
    "\n",
    "# Introduce some missing values\n",
    "df_imputation.loc[[2, 5, 10], 'numerical_feature_1'] = np.nan\n",
    "df_imputation.loc[[7, 15], 'numerical_feature_2'] = np.nan\n",
    "df_imputation.loc[[3, 12, 18], 'categorical_feature'] = np.nan\n",
    "\n",
    "print(\"\\nOriginal DataFrame with missing values:\")\n",
    "print(df_imputation)\n",
    "print(\"\\nMissing values count per column:\")\n",
    "print(df_imputation.isnull().sum())\n",
    "\n",
    "print(\"\\n--- Performing Simple Imputation ---\")\n",
    "\n",
    "# Impute numerical features with the mean\n",
    "numerical_cols = df_imputation.select_dtypes(include=np.number).columns\n",
    "for col in numerical_cols:\n",
    "    mean_val = df_imputation[col].mean()\n",
    "    df_imputation[col].fillna(mean_val, inplace=True)\n",
    "    print(f\"Imputed numerical column '{col}' with its mean: {mean_val:.2f}\")\n",
    "\n",
    "# Impute categorical features with the mode\n",
    "categorical_cols = df_imputation.select_dtypes(include='object').columns\n",
    "for col in categorical_cols:\n",
    "    # .mode()[0] is used because .mode() can return multiple modes if they have the same frequency\n",
    "    mode_val = df_imputation[col].mode()[0]\n",
    "    df_imputation[col].fillna(mode_val, inplace=True)\n",
    "    print(f\"Imputed categorical column '{col}' with its mode: {mode_val}\")\n",
    "\n",
    "print(\"\\nDataFrame after simple imputation:\")\n",
    "print(df_imputation)\n",
    "print(\"\\nMissing values count after imputation:\")\n",
    "print(df_imputation.isnull().sum())\n",
    "\n",
    "print(\"\\nSimple imputation (mean for numerical, mode for categorical) complete.\")\n",
    "\n",
    "\n",
    "# --- New Section: Task 2: Feature Scaling - Min-Max Normalization ---\n",
    "print(\"\\n\\n--- Task 2: Feature Scaling - Min-Max Normalization ---\")\n",
    "\n",
    "print(\"Simulating a numerical feature for Min-Max Normalization...\")\n",
    "\n",
    "# Simulate a numerical feature\n",
    "np.random.seed(41)\n",
    "feature_to_normalize = np.random.randint(50, 500, size=30).reshape(-1, 1) # Reshape for MinMaxScaler\n",
    "df_minmax = pd.DataFrame(feature_to_normalize, columns=['Original_Feature'])\n",
    "\n",
    "print(\"\\nOriginal Feature (first 5 rows):\")\n",
    "print(df_minmax.head())\n",
    "print(f\"Original Min: {df_minmax['Original_Feature'].min()}, Max: {df_minmax['Original_Feature'].max()}\")\n",
    "\n",
    "print(\"\\n--- Applying Min-Max Normalization ---\")\n",
    "\n",
    "# Initialize MinMaxScaler to scale to [0, 1]\n",
    "min_max_scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "\n",
    "# Fit and transform the feature\n",
    "normalized_feature = min_max_scaler.fit_transform(df_minmax[['Original_Feature']])\n",
    "df_minmax['Normalized_Feature'] = normalized_feature\n",
    "\n",
    "print(\"\\nFeature after Min-Max Normalization (first 5 rows):\")\n",
    "print(df_minmax.head())\n",
    "print(f\"Normalized Min: {df_minmax['Normalized_Feature'].min():.2f}, Max: {df_minmax['Normalized_Feature'].max():.2f}\")\n",
    "\n",
    "print(\"\\nMin-Max Normalization complete. The feature is now scaled to the range [0, 1].\")\n",
    "\n",
    "\n",
    "# --- New Section: Task 3: Handling Missing Values - Drop Missing Values ---\n",
    "print(\"\\n\\n--- Task 3: Handling Missing Values - Drop Missing Values ---\")\n",
    "\n",
    "print(\"Simulating a DataFrame with missing values for dropping demonstration...\")\n",
    "\n",
    "# Create a DataFrame with missing values\n",
    "np.random.seed(42)\n",
    "data_drop = {\n",
    "    'col_A': [1, 2, np.nan, 4, 5],\n",
    "    'col_B': [6, np.nan, 8, 9, 10],\n",
    "    'col_C': [11, 12, 13, np.nan, 15]\n",
    "}\n",
    "df_drop = pd.DataFrame(data_drop)\n",
    "\n",
    "print(\"\\nOriginal DataFrame with missing values:\")\n",
    "print(df_drop)\n",
    "print(\"\\nMissing values count per column:\")\n",
    "print(df_drop.isnull().sum())\n",
    "print(f\"Original DataFrame shape: {df_drop.shape}\")\n",
    "\n",
    "print(\"\\n--- Removing Rows with Missing Values ---\")\n",
    "\n",
    "# Drop rows that contain any missing values\n",
    "df_dropped = df_drop.dropna()\n",
    "\n",
    "print(\"\\nDataFrame after dropping rows with missing values:\")\n",
    "print(df_dropped)\n",
    "print(f\"DataFrame shape after dropping missing values: {df_dropped.shape}\")\n",
    "print(\"\\nMissing values count after dropping:\")\n",
    "print(df_dropped.isnull().sum())\n",
    "\n",
    "print(\"\\nDropping missing values complete. Rows containing any NaN values have been removed.\")\n",
    "\n",
    "\n",
    "# --- New Section: Task 4: Feature Scaling - Standardization ---\n",
    "print(\"\\n\\n--- Task 4: Feature Scaling - Standardization ---\")\n",
    "\n",
    "print(\"Simulating a numerical feature for Standardization...\")\n",
    "\n",
    "# Simulate a numerical feature\n",
    "np.random.seed(43)\n",
    "feature_to_standardize = np.random.normal(loc=150, scale=25, size=30).reshape(-1, 1) # Reshape for StandardScaler\n",
    "df_standard = pd.DataFrame(feature_to_standardize, columns=['Original_Feature'])\n",
    "\n",
    "print(\"\\nOriginal Feature (first 5 rows):\")\n",
    "print(df_standard.head())\n",
    "print(f\"Original Mean: {df_standard['Original_Feature'].mean():.2f}, Std Dev: {df_standard['Original_Feature'].std():.2f}\")\n",
    "\n",
    "print(\"\\n--- Applying Standardization ---\")\n",
    "\n",
    "# Initialize StandardScaler\n",
    "standard_scaler = StandardScaler()\n",
    "\n",
    "# Fit and transform the feature\n",
    "standardized_feature = standard_scaler.fit_transform(df_standard[['Original_Feature']])\n",
    "df_standard['Standardized_Feature'] = standardized_feature\n",
    "\n",
    "print(\"\\nFeature after Standardization (first 5 rows):\")\n",
    "print(df_standard.head())\n",
    "print(f\"Standardized Mean: {df_standard['Standardized_Feature'].mean():.2f}, Std Dev: {df_standard['Standardized_Feature'].std():.2f}\")\n",
    "\n",
    "print(\"\\nStandardization complete. The feature now has a mean of approximately 0 and a standard deviation of approximately 1.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
