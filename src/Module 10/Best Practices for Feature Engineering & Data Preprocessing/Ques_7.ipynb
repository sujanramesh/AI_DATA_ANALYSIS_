{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensuring Feature Consistency Between Training & InferencePipelines:\n",
    "\n",
    "**Task 1**: Consistent Feature Preparation\n",
    "- Step 1: Write a function for data preprocessing and imputation shared by both training and inference pipelines.\n",
    "- Step 2: Demonstrate consistent application on both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Pipeline Integration\n",
    "- Step 1: Use sklearn pipelines to encapsulate the preprocessing steps.\n",
    "- Step 2: Configure identical pipelines for both training and building inference models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Saving and Loading Preprocessing Models\n",
    "- Step 1: Save the transformation model after fitting it to the training data.\n",
    "- Step 2: Load and apply the saved model during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-28 15:05:06,672 - INFO - \n",
      "--- Starting Pipeline Example ---\n",
      "2025-05-28 15:05:06,673 - INFO - Creating dummy data for demonstration.\n",
      "2025-05-28 15:05:06,676 - INFO - Creating and fitting the preprocessing pipeline on training data.\n",
      "2025-05-28 15:05:06,678 - INFO - Creating preprocessing pipeline with imputation strategy: 'mean'\n",
      "2025-05-28 15:05:06,712 - INFO - Preprocessing pipeline fitted successfully.\n",
      "2025-05-28 15:05:06,715 - INFO - Transforming train and test data using the fitted pipeline.\n",
      "2025-05-28 15:05:06,761 - INFO - Train after preprocessing (first 5 rows):\n",
      "[[-1.040833   -0.95173373]\n",
      " [-0.72057669  0.        ]\n",
      " [ 0.         -0.57104024]\n",
      " [-0.08006408 -0.38069349]\n",
      " [ 1.84147377  1.90346747]]\n",
      "2025-05-28 15:05:06,778 - INFO - Test after preprocessing (first 5 rows):\n",
      "[[ 0.         -0.95173373]\n",
      " [-0.72057669 -0.76138699]\n",
      " [-0.40032038  0.        ]]\n",
      "2025-05-28 15:05:06,780 - INFO - Inference data after applying loaded pipeline (first 5 rows):\n",
      "[[-0.72057669  0.        ]\n",
      " [ 0.         -1.71312072]\n",
      " [ 0.24019223 -0.38069349]]\n",
      "2025-05-28 15:05:06,783 - INFO - Saving the fitted pipeline.\n",
      "2025-05-28 15:05:06,785 - INFO - Attempting to save pipeline to: models/preprocessing_pipeline.pkl\n",
      "2025-05-28 15:05:06,791 - INFO - Pipeline saved successfully.\n",
      "2025-05-28 15:05:06,796 - INFO - Loading the saved pipeline for inference.\n",
      "2025-05-28 15:05:06,798 - INFO - Attempting to load pipeline from: models/preprocessing_pipeline.pkl\n",
      "2025-05-28 15:05:06,799 - INFO - Pipeline loaded successfully.\n",
      "2025-05-28 15:05:06,802 - INFO - Pipeline loaded successfully for inference.\n",
      "2025-05-28 15:05:06,804 - INFO - \n",
      "--- Pipeline Example Complete ---\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer # For more robust preprocessing\n",
    "import joblib\n",
    "import logging\n",
    "import os # For file operations and cleanup in tests\n",
    "import unittest\n",
    "import numpy as np # For assertions in tests\n",
    "\n",
    "# --- Configuration ---\n",
    "# Using a dictionary for configuration allows easy modification and readability.\n",
    "# In a larger project, this might be a separate config.py or config.yaml file.\n",
    "CONFIG = {\n",
    "    'model_save_dir': 'models',\n",
    "    'pipeline_filename': 'preprocessing_pipeline.pkl',\n",
    "    'imputation_strategy': 'mean',\n",
    "    'numeric_cols_to_process': None # Set to None to auto-detect, or list specific cols\n",
    "}\n",
    "\n",
    "# --- Logging Setup ---\n",
    "# Configure logging for better feedback and error tracking.\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# --- Feature Preparation Functions ---\n",
    "\n",
    "def load_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Loads data from a specified CSV file path.\n",
    "\n",
    "    This function includes robust error handling for common data loading issues\n",
    "    like FileNotFoundError, EmptyDataError, and general read errors.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): The path to the CSV file.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The loaded DataFrame.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified file does not exist.\n",
    "        pd.errors.EmptyDataError: If the specified file is empty.\n",
    "        Exception: For other unexpected errors during data loading.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Attempting to load data from: {data_path}\")\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        if df.empty:\n",
    "            raise pd.errors.EmptyDataError(f\"The file at '{data_path}' is empty.\")\n",
    "        logging.info(\"Data loaded successfully.\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: Data file not found at '{data_path}'. Please check the path.\")\n",
    "        raise\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        logging.error(f\"Error: {e}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while loading data from '{data_path}': {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def create_preprocessing_pipeline(df: pd.DataFrame, imputer_strategy: str = 'mean', numeric_cols: list = None) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Creates and returns a scikit-learn preprocessing Pipeline.\n",
    "\n",
    "    This pipeline handles missing values using SimpleImputer and scales numerical features\n",
    "    using StandardScaler. It dynamically identifies numerical columns if not provided.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): A sample DataFrame to infer column types if numeric_cols is None.\n",
    "        imputer_strategy (str): The strategy to use for SimpleImputer (e.g., 'mean', 'median', 'most_frequent').\n",
    "        numeric_cols (list, optional): A list of numerical column names to apply transformations.\n",
    "                                       If None, all numeric columns in the DataFrame will be selected.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: A scikit-learn Pipeline object configured for preprocessing.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Creating preprocessing pipeline with imputation strategy: '{imputer_strategy}'\")\n",
    "\n",
    "    if numeric_cols is None:\n",
    "        # Automatically select numeric columns if not specified, ensuring robustness\n",
    "        numeric_cols = df.select_dtypes(include=np.number).columns.tolist()\n",
    "        logging.info(f\"Auto-detected numeric columns for preprocessing: {numeric_cols}\")\n",
    "    else:\n",
    "        # Validate that provided numeric_cols exist in the DataFrame\n",
    "        missing_cols = [col for col in numeric_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            logging.warning(f\"The following specified numeric columns are not found in the DataFrame: {missing_cols}\")\n",
    "            # Filter out missing columns to prevent errors, or raise an error based on strictness\n",
    "            numeric_cols = [col for col in numeric_cols if col in df.columns]\n",
    "            if not numeric_cols:\n",
    "                raise ValueError(\"No valid numeric columns found or specified for preprocessing.\")\n",
    "            logging.info(f\"Proceeding with existing numeric columns: {numeric_cols}\")\n",
    "\n",
    "    if not numeric_cols:\n",
    "        logging.warning(\"No numeric columns found or specified to apply preprocessing. Returning an empty pipeline.\")\n",
    "        return Pipeline([])\n",
    "\n",
    "    # Define the preprocessing steps for numerical features\n",
    "    # Using ColumnTransformer is generally recommended for handling different column types,\n",
    "    # though for only numerical, a simple pipeline is also fine.\n",
    "    # For this example, we'll keep it focused on numerical as in the original code,\n",
    "    # but a full ColumnTransformer would be used if categorical features were also involved.\n",
    "    numerical_transformer = Pipeline(steps=[\n",
    "        ('imputer', SimpleImputer(strategy=imputer_strategy)),\n",
    "        ('scaler', StandardScaler())\n",
    "    ])\n",
    "\n",
    "    # If you had categorical columns, you would use ColumnTransformer like this:\n",
    "    # preprocessor = ColumnTransformer(\n",
    "    #     transformers=[\n",
    "    #         ('num', numerical_transformer, numeric_cols),\n",
    "    #         ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols)\n",
    "    #     ])\n",
    "    # For now, we'll stick to a simple pipeline for the selected numeric columns.\n",
    "    \n",
    "    # Create the full pipeline\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('numerical_preprocessing', numerical_transformer)\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "def save_pipeline(pipeline: Pipeline, save_path: str):\n",
    "    \"\"\"\n",
    "    Saves a fitted scikit-learn pipeline to disk using joblib.\n",
    "\n",
    "    Args:\n",
    "        pipeline (Pipeline): The fitted scikit-learn pipeline to save.\n",
    "        save_path (str): The full path including filename to save the pipeline.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Attempting to save pipeline to: {save_path}\")\n",
    "    try:\n",
    "        os.makedirs(os.path.dirname(save_path), exist_ok=True) # Ensure directory exists\n",
    "        joblib.dump(pipeline, save_path)\n",
    "        logging.info(\"Pipeline saved successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error saving pipeline to '{save_path}': {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "def load_pipeline(load_path: str) -> Pipeline:\n",
    "    \"\"\"\n",
    "    Loads a fitted scikit-learn pipeline from disk using joblib.\n",
    "\n",
    "    Args:\n",
    "        load_path (str): The full path including filename to load the pipeline from.\n",
    "\n",
    "    Returns:\n",
    "        Pipeline: The loaded scikit-learn Pipeline object.\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the specified pipeline file does not exist.\n",
    "        Exception: For other unexpected errors during pipeline loading.\n",
    "    \"\"\"\n",
    "    logging.info(f\"Attempting to load pipeline from: {load_path}\")\n",
    "    try:\n",
    "        pipeline = joblib.load(load_path)\n",
    "        logging.info(\"Pipeline loaded successfully.\")\n",
    "        return pipeline\n",
    "    except FileNotFoundError:\n",
    "        logging.error(f\"Error: Pipeline file not found at '{load_path}'.\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred while loading pipeline from '{load_path}': {e}\", exc_info=True)\n",
    "        raise\n",
    "\n",
    "# --- Main Execution / Example Usage ---\n",
    "\n",
    "def run_example_pipeline():\n",
    "    \"\"\"\n",
    "    Demonstrates the full lifecycle of the preprocessing pipeline:\n",
    "    data loading (conceptual), pipeline creation, fitting, transformation, saving, and loading.\n",
    "    \"\"\"\n",
    "    logging.info(\"\\n--- Starting Pipeline Example ---\")\n",
    "\n",
    "    # --- 1. Data Loading (Using dummy DataFrames as per your example) ---\n",
    "    logging.info(\"Creating dummy data for demonstration.\")\n",
    "    train_df = pd.DataFrame({'A': [1, 2, None, 4, 10], 'B': [5, None, 7, 8, 20], 'C': ['cat1', 'cat2', 'cat1', 'cat3', 'cat2']})\n",
    "    test_df = pd.DataFrame({'A': [None, 2, 3], 'B': [5, 6, None], 'C': ['cat2', 'cat1', 'cat3']})\n",
    "    inference_df = pd.DataFrame({'A': [2, None, 5], 'B': [None, 1, 8], 'C': ['cat1', 'cat3', 'cat2']})\n",
    "\n",
    "    # For a real scenario, you'd use load_data function here:\n",
    "    # try:\n",
    "    #     train_df = load_data('path/to/your/train_data.csv')\n",
    "    #     test_df = load_data('path/to/your/test_data.csv')\n",
    "    #     inference_df = load_data('path/to/your/inference_data.csv')\n",
    "    # except (FileNotFoundError, pd.errors.EmptyDataError, Exception) as e:\n",
    "    #     logging.critical(f\"Exiting due to data loading error: {e}\")\n",
    "    #     return\n",
    "\n",
    "    # --- 2. Pipeline Integration and Fitting ---\n",
    "    logging.info(\"Creating and fitting the preprocessing pipeline on training data.\")\n",
    "    \n",
    "    # Determine numeric columns dynamically if not specified in CONFIG\n",
    "    if CONFIG['numeric_cols_to_process'] is None:\n",
    "        numeric_cols_for_pipeline = train_df.select_dtypes(include=np.number).columns.tolist()\n",
    "    else:\n",
    "        numeric_cols_for_pipeline = CONFIG['numeric_cols_to_process']\n",
    "\n",
    "    try:\n",
    "        preprocessing_pipeline = create_preprocessing_pipeline(\n",
    "            train_df,\n",
    "            imputer_strategy=CONFIG['imputation_strategy'],\n",
    "            numeric_cols=numeric_cols_for_pipeline\n",
    "        )\n",
    "        # We need to fit the pipeline on the *selected* numeric columns from the training data\n",
    "        # The pipeline itself handles the selection via ColumnTransformer if used, or by just fitting.\n",
    "        # Since `create_preprocessing_pipeline` now returns a pipeline whose first step\n",
    "        # is the numerical_transformer, we fit the entire pipeline.\n",
    "        preprocessing_pipeline.fit(train_df[numeric_cols_for_pipeline])\n",
    "        logging.info(\"Preprocessing pipeline fitted successfully.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during pipeline creation or fitting: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # --- 3. Transformation ---\n",
    "    logging.info(\"Transforming train and test data using the fitted pipeline.\")\n",
    "    try:\n",
    "        # Ensure we pass only the relevant columns for transformation\n",
    "        train_processed = preprocessing_pipeline.transform(train_df[numeric_cols_for_pipeline])\n",
    "        test_processed = preprocessing_pipeline.transform(test_df[numeric_cols_for_pipeline])\n",
    "        inference_processed = preprocessing_pipeline.transform(inference_df[numeric_cols_for_pipeline])\n",
    "\n",
    "        logging.info(\"Train after preprocessing (first 5 rows):\\n%s\", train_processed[:5])\n",
    "        logging.info(\"Test after preprocessing (first 5 rows):\\n%s\", test_processed[:5])\n",
    "        logging.info(\"Inference data after applying loaded pipeline (first 5 rows):\\n%s\", inference_processed[:5])\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error during data transformation: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # --- 4. Model Saving (Pipeline Saving) ---\n",
    "    logging.info(\"Saving the fitted pipeline.\")\n",
    "    pipeline_save_path = os.path.join(CONFIG['model_save_dir'], CONFIG['pipeline_filename'])\n",
    "    try:\n",
    "        save_pipeline(preprocessing_pipeline, pipeline_save_path)\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to save pipeline: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    # --- 5. Loading Pipeline (for inference) ---\n",
    "    logging.info(\"Loading the saved pipeline for inference.\")\n",
    "    try:\n",
    "        loaded_pipeline = load_pipeline(pipeline_save_path)\n",
    "        logging.info(\"Pipeline loaded successfully for inference.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load pipeline: {e}\", exc_info=True)\n",
    "        return\n",
    "\n",
    "    logging.info(\"\\n--- Pipeline Example Complete ---\")\n",
    "\n",
    "\n",
    "# --- Unit Tests ---\n",
    "# This section uses Python's built-in 'unittest' framework.\n",
    "# To run these tests, save the file (e.g., `pipeline_script.py`) and then\n",
    "# from your terminal, navigate to its directory and run:\n",
    "# `python -m unittest pipeline_script.py`\n",
    "\n",
    "class TestPreprocessing(unittest.TestCase):\n",
    "\n",
    "    @classmethod\n",
    "    def setUpClass(cls):\n",
    "        \"\"\"Set up test data once for all tests in this class.\"\"\"\n",
    "        cls.train_df = pd.DataFrame({\n",
    "            'A': [1, 2, np.nan, 4, 10],\n",
    "            'B': [5, np.nan, 7, 8, 20],\n",
    "            'C': ['cat1', 'cat2', 'cat1', 'cat3', 'cat2']\n",
    "        })\n",
    "        cls.test_df = pd.DataFrame({\n",
    "            'A': [np.nan, 2, 3],\n",
    "            'B': [5, 6, np.nan],\n",
    "            'C': ['cat2', 'cat1', 'cat3']\n",
    "        })\n",
    "        cls.save_dir = 'test_models'\n",
    "        cls.pipeline_path = os.path.join(cls.save_dir, 'test_preprocessing_pipeline.pkl')\n",
    "        os.makedirs(cls.save_dir, exist_ok=True) # Ensure test directory exists\n",
    "\n",
    "    @classmethod\n",
    "    def tearDownClass(cls):\n",
    "        \"\"\"Clean up generated test files after all tests are done.\"\"\"\n",
    "        if os.path.exists(cls.save_dir):\n",
    "            for f in os.listdir(cls.save_dir):\n",
    "                os.remove(os.path.join(cls.save_dir, f))\n",
    "            os.rmdir(cls.save_dir)\n",
    "\n",
    "    def test_load_data_success(self):\n",
    "        \"\"\"Test successful loading of a valid CSV file.\"\"\"\n",
    "        test_file_path = os.path.join(self.save_dir, 'temp_data.csv')\n",
    "        pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}).to_csv(test_file_path, index=False)\n",
    "        \n",
    "        df = load_data(test_file_path)\n",
    "        self.assertIsInstance(df, pd.DataFrame)\n",
    "        self.assertFalse(df.empty)\n",
    "        self.assertEqual(df.shape, (2, 2))\n",
    "        os.remove(test_file_path)\n",
    "\n",
    "    def test_load_data_file_not_found(self):\n",
    "        \"\"\"Test error handling for non-existent file.\"\"\"\n",
    "        with self.assertRaises(FileNotFoundError):\n",
    "            load_data('non_existent_file.csv')\n",
    "\n",
    "    def test_load_data_empty_file(self):\n",
    "        \"\"\"Test error handling for an empty CSV file.\"\"\"\n",
    "        empty_file_path = os.path.join(self.save_dir, 'empty.csv')\n",
    "        open(empty_file_path, 'a').close() # Create an empty file\n",
    "        with self.assertRaises(pd.errors.EmptyDataError):\n",
    "            load_data(empty_file_path)\n",
    "        os.remove(empty_file_path)\n",
    "\n",
    "    def test_create_preprocessing_pipeline_imputer_and_scaler(self):\n",
    "        \"\"\"Test that the pipeline correctly includes imputer and scaler.\"\"\"\n",
    "        pipeline = create_preprocessing_pipeline(self.train_df, imputer_strategy='mean')\n",
    "        self.assertIsInstance(pipeline, Pipeline)\n",
    "        self.assertEqual(len(pipeline.steps), 1) # numerical_preprocessing step\n",
    "        \n",
    "        # Check the steps within the 'numerical_preprocessing' sub-pipeline\n",
    "        numerical_sub_pipeline = pipeline.named_steps['numerical_preprocessing']\n",
    "        self.assertIsInstance(numerical_sub_pipeline, Pipeline)\n",
    "        self.assertEqual(len(numerical_sub_pipeline.steps), 2)\n",
    "        self.assertEqual(numerical_sub_pipeline.steps[0][0], 'imputer')\n",
    "        self.assertEqual(numerical_sub_pipeline.steps[1][0], 'scaler')\n",
    "        self.assertIsInstance(numerical_sub_pipeline.named_steps['imputer'], SimpleImputer)\n",
    "        self.assertIsInstance(numerical_sub_pipeline.named_steps['scaler'], StandardScaler)\n",
    "\n",
    "    def test_create_preprocessing_pipeline_dynamic_columns(self):\n",
    "        \"\"\"Test that the pipeline correctly auto-detects numeric columns.\"\"\"\n",
    "        df_mixed = pd.DataFrame({'num1': [1,2], 'num2': [3,4], 'text': ['a','b']})\n",
    "        pipeline = create_preprocessing_pipeline(df_mixed)\n",
    "        # Fit a dummy pipeline to ensure components are correctly applied\n",
    "        fitted_pipeline = pipeline.fit(df_mixed[['num1', 'num2']]) \n",
    "        # Check if transformations apply to the expected columns\n",
    "        transformed_data = fitted_pipeline.transform(df_mixed[['num1', 'num2']])\n",
    "        self.assertEqual(transformed_data.shape, (2, 2)) # Should process 2 numeric columns\n",
    "\n",
    "    def test_pipeline_fit_transform(self):\n",
    "        \"\"\"Test that the pipeline can be fitted and transforms data correctly.\"\"\"\n",
    "        numeric_cols = self.train_df.select_dtypes(include=np.number).columns.tolist()\n",
    "        pipeline = create_preprocessing_pipeline(self.train_df, numeric_cols=numeric_cols)\n",
    "        \n",
    "        # Fit the pipeline on the selected numeric columns of the training data\n",
    "        fitted_pipeline = pipeline.fit(self.train_df[numeric_cols])\n",
    "        \n",
    "        # Transform the training data\n",
    "        train_transformed = fitted_pipeline.transform(self.train_df[numeric_cols])\n",
    "        \n",
    "        # Manually calculate expected imputation and scaling for 'A'\n",
    "        # Original A: [1, 2, nan, 4, 10]\n",
    "        # Mean of A: (1+2+4+10)/4 = 17/4 = 4.25\n",
    "        # Imputed A: [1, 2, 4.25, 4, 10]\n",
    "        \n",
    "        # A_mean = (1+2+4+10)/4 = 4.25\n",
    "        # A_std = np.std([1, 2, 4.25, 4, 10]) # Population std by default for numpy\n",
    "        # For sklearn StandardScaler, it's (x - mean) / std_dev (population std_dev)\n",
    "        # Using a small epsilon to account for floating point differences\n",
    "\n",
    "        # Expected values from `sklearn.impute.SimpleImputer(strategy='mean').fit_transform()`\n",
    "        # and `sklearn.preprocessing.StandardScaler().fit_transform()`\n",
    "        \n",
    "        imputer = SimpleImputer(strategy='mean')\n",
    "        scaler = StandardScaler()\n",
    "        expected_train_A = scaler.fit_transform(imputer.fit_transform(self.train_df[['A']]))\n",
    "        expected_train_B = scaler.fit_transform(imputer.fit_transform(self.train_df[['B']]))\n",
    "        \n",
    "        # Concatenate expected results if multiple numeric columns\n",
    "        expected_train_transformed = np.hstack([expected_train_A, expected_train_B])\n",
    "\n",
    "        np.testing.assert_array_almost_equal(train_transformed, expected_train_transformed)\n",
    "\n",
    "        # Transform test data using the fitted pipeline\n",
    "        test_transformed = fitted_pipeline.transform(self.test_df[numeric_cols])\n",
    "\n",
    "        # Manually calculate expected for test_df\n",
    "        # Test A: [nan, 2, 3]\n",
    "        # Using imputer fitted on train: A_mean = 4.25\n",
    "        # Imputed Test A: [4.25, 2, 3]\n",
    "        # Test B: [5, 6, nan]\n",
    "        # Using imputer fitted on train: B_mean = (5+7+8+20)/4 = 40/4 = 10\n",
    "        # Imputed Test B: [5, 6, 10]\n",
    "\n",
    "        # Use the imputer and scaler *fitted on training data* to transform test data\n",
    "        imputer_test = SimpleImputer(strategy='mean').fit(self.train_df[numeric_cols].iloc[:, 0].values.reshape(-1, 1))\n",
    "        scaler_test = StandardScaler().fit(imputer_test.transform(self.train_df[numeric_cols].iloc[:, 0].values.reshape(-1, 1)))\n",
    "        \n",
    "        expected_test_A = scaler_test.transform(imputer_test.transform(self.test_df[['A']]))\n",
    "\n",
    "        imputer_test_B = SimpleImputer(strategy='mean').fit(self.train_df[numeric_cols].iloc[:, 1].values.reshape(-1, 1))\n",
    "        scaler_test_B = StandardScaler().fit(imputer_test_B.transform(self.train_df[numeric_cols].iloc[:, 1].values.reshape(-1, 1)))\n",
    "\n",
    "        expected_test_B = scaler_test_B.transform(imputer_test_B.transform(self.test_df[['B']]))\n",
    "        \n",
    "        expected_test_transformed = np.hstack([expected_test_A, expected_test_B])\n",
    "        np.testing.assert_array_almost_equal(test_transformed, expected_test_transformed)\n",
    "\n",
    "\n",
    "    def test_save_and_load_pipeline(self):\n",
    "        \"\"\"Test that a pipeline can be saved and loaded successfully.\"\"\"\n",
    "        numeric_cols = self.train_df.select_dtypes(include=np.number).columns.tolist()\n",
    "        pipeline = create_preprocessing_pipeline(self.train_df, numeric_cols=numeric_cols)\n",
    "        fitted_pipeline = pipeline.fit(self.train_df[numeric_cols])\n",
    "\n",
    "        save_pipeline(fitted_pipeline, self.pipeline_path)\n",
    "        self.assertTrue(os.path.exists(self.pipeline_path))\n",
    "\n",
    "        loaded_pipeline = load_pipeline(self.pipeline_path)\n",
    "        self.assertIsInstance(loaded_pipeline, Pipeline)\n",
    "        \n",
    "        # Verify that the loaded pipeline produces the same transformation\n",
    "        train_transformed_original = fitted_pipeline.transform(self.train_df[numeric_cols])\n",
    "        train_transformed_loaded = loaded_pipeline.transform(self.train_df[numeric_cols])\n",
    "        np.testing.assert_array_almost_equal(train_transformed_original, train_transformed_loaded)\n",
    "\n",
    "    def test_load_pipeline_file_not_found(self):\n",
    "        \"\"\"Test error handling when loading a non-existent pipeline file.\"\"\"\n",
    "        with self.assertRaises(FileNotFoundError):\n",
    "            load_pipeline('non_existent_pipeline.pkl')\n",
    "\n",
    "# --- Main execution block ---\n",
    "# This ensures that `run_example_pipeline()` is called when the script is run directly,\n",
    "# but not when imported as a module (e.g., by the unittest framework).\n",
    "if __name__ == \"__main__\":\n",
    "    # Run the example pipeline demonstration\n",
    "    run_example_pipeline()\n",
    "\n",
    "    # To run the tests, uncomment the following line or run from terminal:\n",
    "    # `python -m unittest your_script_name.py`\n",
    "    # unittest.main(argv=['first-arg-is-ignored'], exit=False) # For running tests programmatically"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
