{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Handling Schema Mismatches using Spark\n",
    "**Description**: Use Apache Spark to address schema mismatches by transforming data to match\n",
    "the expected schema.\n",
    "\n",
    "**Steps**:\n",
    "1. Create Spark session\n",
    "2. Load dataframe\n",
    "3. Define the expected schema\n",
    "4. Handle schema mismatches\n",
    "5. Show corrected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Detect and Correct Incomplete Data in ETL\n",
    "**Description**: Use Python and Pandas to detect incomplete data in an ETL process and fill\n",
    "missing values with estimates.\n",
    "\n",
    "**Steps**:\n",
    "1. Detect incomplete data\n",
    "2. Fill missing values\n",
    "3. Report changes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# This script demonstrates how to handle schema mismatches using Apache Spark.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# To run this script, you need to have Apache Spark installed.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# If you don't have it, you can install pyspark using: pip install pyspark\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StructType, StructField, StringType, IntegerType, DoubleType\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m col, lit\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "# This script demonstrates how to handle schema mismatches using Apache Spark.\n",
    "\n",
    "# To run this script, you need to have Apache Spark installed.\n",
    "# If you don't have it, you can install pyspark using: pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# Task 1: Handling Schema Mismatches using Spark\n",
    "# Description: Use Apache Spark to address schema mismatches by transforming data\n",
    "#              to match the expected schema.\n",
    "\n",
    "# Step 1: Create Spark session\n",
    "# This is the entry point for using Spark functionality.\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SchemaMismatchHandler\") \\\n",
    "        .getOrCreate()\n",
    "    print(\"Step 1: Spark Session created successfully.\")\n",
    "\n",
    "    # Step 2: Load dataframe (simulated)\n",
    "    # We'll create a dummy DataFrame with some schema inconsistencies.\n",
    "    # For example, 'age' might come in as a string, or 'salary' as an integer when float is expected.\n",
    "    data = [\n",
    "        (1, \"Alice\", \"30\", \"New York\", \"50000.0\"),\n",
    "        (2, \"Bob\", \"25\", \"Los Angeles\", \"60000\"),\n",
    "        (3, \"Charlie\", \"thirty\", \"Chicago\", \"75000.50\"), # 'thirty' is a mismatch for IntegerType\n",
    "        (4, \"David\", \"35\", \"Houston\", \"80000\"),\n",
    "        (5, \"Eve\", \"28\", \"Miami\", \"invalid_salary\") # 'invalid_salary' is a mismatch for DoubleType\n",
    "    ]\n",
    "    # Define a preliminary schema for the raw data to load it into Spark\n",
    "    raw_schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", StringType(), True), # Intentionally StringType to simulate mismatch\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"salary\", StringType(), True) # Intentionally StringType to simulate mismatch\n",
    "    ])\n",
    "    df = spark.createDataFrame(data, schema=raw_schema)\n",
    "    print(\"\\nStep 2: Original DataFrame loaded.\")\n",
    "    df.printSchema()\n",
    "    df.show()\n",
    "\n",
    "    # Step 3: Define the expected schema\n",
    "    # This is the target schema we want our data to conform to.\n",
    "    expected_schema = StructType([\n",
    "        StructField(\"id\", IntegerType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True),\n",
    "        StructField(\"city\", StringType(), True),\n",
    "        StructField(\"salary\", DoubleType(), True)\n",
    "    ])\n",
    "    print(\"\\nStep 3: Expected Schema defined.\")\n",
    "    print(expected_schema)\n",
    "\n",
    "    # Step 4: Handle schema mismatches\n",
    "    # Iterate through the expected schema and cast columns.\n",
    "    # If a cast fails (e.g., \"thirty\" to Integer), Spark will typically put null.\n",
    "    # We can also add more robust error handling or default values if needed.\n",
    "    corrected_df = df\n",
    "    for field in expected_schema.fields:\n",
    "        column_name = field.name\n",
    "        expected_type = field.dataType\n",
    "\n",
    "        # Check if the column exists in the DataFrame\n",
    "        if column_name in corrected_df.columns:\n",
    "            # Cast the column to the expected type\n",
    "            # .cast() handles type conversions. If conversion fails, it results in null.\n",
    "            corrected_df = corrected_df.withColumn(column_name, col(column_name).cast(expected_type))\n",
    "        else:\n",
    "            # If a column is missing from the source data but is in the expected schema,\n",
    "            # add it with null values and the expected type.\n",
    "            print(f\"Warning: Column '{column_name}' missing in source data. Adding with nulls.\")\n",
    "            corrected_df = corrected_df.withColumn(column_name, lit(None).cast(expected_type))\n",
    "\n",
    "    # Step 5: Show corrected data\n",
    "    print(\"\\nStep 4 & 5: Schema mismatches handled and corrected data shown.\")\n",
    "    print(\"\\nCorrected DataFrame Schema:\")\n",
    "    corrected_df.printSchema()\n",
    "    print(\"\\nCorrected DataFrame Data:\")\n",
    "    corrected_df.show()\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during Spark schema mismatch handling: {e}\")\n",
    "finally:\n",
    "    # Stop the Spark session if it was created\n",
    "    if 'spark' in locals() and spark:\n",
    "        spark.stop()\n",
    "        print(\"\\nSpark Session stopped.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
