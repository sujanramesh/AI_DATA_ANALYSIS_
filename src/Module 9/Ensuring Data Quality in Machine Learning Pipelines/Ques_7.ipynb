{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering Best Practices: Handling Text Data\n",
    "**Question**: Load a dataset with text data (e.g., SMS Spam Collection), perform text\n",
    "preprocessing, and extract numerical features using TF-IDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'nltk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstem\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PorterStemmer\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'nltk'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data (if not already downloaded)\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('stopwords')\n",
    "try:\n",
    "    nltk.data.find('stemmers/porter')\n",
    "except nltk.downloader.DownloadError:\n",
    "    nltk.download('punkt') # punkt is needed for tokenization, which stemming implicitly uses\n",
    "\n",
    "\n",
    "print(\"Simulating concept drift and preparing visualization...\")\n",
    "\n",
    "# --- Parameters for Concept Drift Simulation ---\n",
    "num_time_periods = 50  # Number of simulated time periods\n",
    "samples_per_period = 100 # Number of data points in each time period\n",
    "base_probability = 0.3  # Initial probability of the binary target (e.g., P(Y=1))\n",
    "drift_magnitude = 0.01 # How much the probability changes per period\n",
    "drift_start_period = 15 # When the drift starts\n",
    "drift_end_period = 40   # When the drift ends\n",
    "\n",
    "# --- Simulate Concept Drift ---\n",
    "# This list will store the true probability of the target variable for each period\n",
    "true_probabilities = []\n",
    "# This list will store the observed frequency of the target variable for each period\n",
    "observed_frequencies = []\n",
    "\n",
    "for i in range(num_time_periods):\n",
    "    current_probability = base_probability\n",
    "\n",
    "    # Introduce concept drift between drift_start_period and drift_end_period\n",
    "    if drift_start_period <= i < drift_end_period:\n",
    "        # Gradually increase the probability over time\n",
    "        current_probability = base_probability + (i - drift_start_period) * drift_magnitude\n",
    "        # Ensure probability stays within [0, 1]\n",
    "        current_probability = np.clip(current_probability, 0.0, 1.0)\n",
    "    elif i >= drift_end_period:\n",
    "        # Keep the probability constant after the drift ends (at its peak value)\n",
    "        current_probability = base_probability + (drift_end_period - drift_start_period) * drift_magnitude\n",
    "        current_probability = np.clip(current_probability, 0.0, 1.0)\n",
    "\n",
    "    true_probabilities.append(current_probability)\n",
    "\n",
    "    # Simulate binary data for the current period based on current_probability\n",
    "    # np.random.binomial(n, p, size) generates n trials with probability p, repeated 'size' times.\n",
    "    # Here, n=1 for a single binary outcome, and size=samples_per_period for multiple samples.\n",
    "    simulated_data = np.random.binomial(1, current_probability, samples_per_period)\n",
    "\n",
    "    # Calculate the observed frequency (proportion of 1s) in the simulated data\n",
    "    observed_frequency = np.mean(simulated_data)\n",
    "    observed_frequencies.append(observed_frequency)\n",
    "\n",
    "# --- Visualize the Concept Drift ---\n",
    "plt.figure(figsize=(12, 6)) # Set the figure size for better readability\n",
    "plt.plot(range(num_time_periods), true_probabilities, label='True Probability (Simulated Drift)', color='blue', linestyle='--', marker='o', markersize=4)\n",
    "plt.plot(range(num_time_periods), observed_frequencies, label='Observed Frequency', color='red', alpha=0.7, marker='x', markersize=4)\n",
    "\n",
    "plt.title('Simulated Concept Drift in Binary Target Variable Over Time', fontsize=16)\n",
    "plt.xlabel('Time Period', fontsize=12)\n",
    "plt.ylabel('Probability / Frequency of Target (Y=1)', fontsize=12)\n",
    "plt.grid(True, linestyle=':', alpha=0.7) # Add a subtle grid\n",
    "plt.legend(fontsize=10) # Display the legend\n",
    "plt.ylim(0, 1) # Ensure y-axis is between 0 and 1 for probabilities\n",
    "plt.xticks(np.arange(0, num_time_periods + 1, 5)) # Set x-axis ticks for clarity\n",
    "plt.tight_layout() # Adjust layout to prevent labels from overlapping\n",
    "plt.show() # Display the plot\n",
    "\n",
    "print(\"\\nConcept drift simulation and visualization complete. The plot shows how the distribution of the binary target variable changes over time.\")\n",
    "\n",
    "\n",
    "# --- New Section: Detecting & Handling Imbalanced Data ---\n",
    "print(\"\\n\\n--- Detecting & Handling Imbalanced Data: Visualizing Class Imbalance ---\")\n",
    "\n",
    "# Define the URL for the Credit Card Fraud Detection dataset\n",
    "# This dataset is known for its highly imbalanced classes (very few fraudulent transactions).\n",
    "credit_card_fraud_url = \"https://raw.githubusercontent.com/mlg-ulb/fpg/master/datasets/creditcard.csv\"\n",
    "\n",
    "print(\"Loading the Credit Card Fraud Detection dataset...\")\n",
    "try:\n",
    "    # Load the dataset into a pandas DataFrame\n",
    "    # The dataset is comma-separated and has a header.\n",
    "    df_fraud = pd.read_csv(credit_card_fraud_url)\n",
    "    print(\"Credit Card Fraud Detection dataset loaded successfully.\")\n",
    "    print(\"\\nFirst 5 rows of the fraud detection dataset:\")\n",
    "    print(df_fraud.head())\n",
    "\n",
    "    print(\"\\n--- Initial Class Distribution ---\")\n",
    "    # The target variable is 'Class' (0: Non-Fraud, 1: Fraud)\n",
    "    class_distribution = df_fraud['Class'].value_counts()\n",
    "    print(\"Absolute counts of each class:\")\n",
    "    print(class_distribution)\n",
    "\n",
    "    # Calculate and print the percentage of each class\n",
    "    class_percentage = df_fraud['Class'].value_counts(normalize=True) * 100\n",
    "    print(\"\\nPercentage of each class:\")\n",
    "    print(class_percentage.round(2))\n",
    "\n",
    "    # --- Visualize Class Imbalance ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x='Class', data=df_fraud, palette='viridis')\n",
    "    plt.title('Class Distribution in Original Credit Card Fraud Dataset', fontsize=14)\n",
    "    plt.xlabel('Class (0: Non-Fraud, 1: Fraud)', fontsize=12)\n",
    "    plt.ylabel('Number of Transactions', fontsize=12)\n",
    "    plt.xticks(ticks=[0, 1], labels=['Non-Fraud (0)', 'Fraud (1)'])\n",
    "    plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Class imbalance visualized. The plot clearly shows the disproportionate number of non-fraudulent vs. fraudulent transactions.\")\n",
    "\n",
    "    print(\"\\n--- Applying Random Undersampling to Balance the Dataset ---\")\n",
    "\n",
    "    # Separate features (X) and target (y)\n",
    "    X = df_fraud.drop('Class', axis=1)\n",
    "    y = df_fraud['Class']\n",
    "\n",
    "    # Identify indices of majority and minority classes\n",
    "    # Class 0 is the majority class, Class 1 is the minority class\n",
    "    fraud_indices = np.array(df_fraud[df_fraud.Class == 1].index)\n",
    "    non_fraud_indices = np.array(df_fraud[df_fraud.Class == 0].index)\n",
    "\n",
    "    # Determine the number of samples in the minority class (fraudulent transactions)\n",
    "    number_of_fraud_samples = len(fraud_indices)\n",
    "    print(f\"Number of fraudulent transactions (minority class): {number_of_fraud_samples}\")\n",
    "\n",
    "    # Randomly select 'number_of_fraud_samples' from the non-fraudulent class\n",
    "    # np.random.choice is used to pick random indices without replacement\n",
    "    random_non_fraud_indices = np.random.choice(\n",
    "        non_fraud_indices,\n",
    "        number_of_fraud_samples,\n",
    "        replace=False # Important: do not pick the same sample multiple times\n",
    "    )\n",
    "    print(f\"Randomly selected {len(random_non_fraud_indices)} non-fraudulent samples.\")\n",
    "\n",
    "    # Concatenate the indices of the minority class and the randomly selected majority class\n",
    "    undersampled_indices = np.concatenate([fraud_indices, random_non_fraud_indices])\n",
    "\n",
    "    # Create the undersampled DataFrame\n",
    "    df_undersampled = df_fraud.loc[undersampled_indices]\n",
    "\n",
    "    print(\"\\n--- Class Distribution After Random Undersampling ---\")\n",
    "    # Check the class distribution in the undersampled dataset\n",
    "    undersampled_class_distribution = df_undersampled['Class'].value_counts()\n",
    "    print(\"Absolute counts of each class after undersampling:\")\n",
    "    print(undersampled_class_distribution)\n",
    "\n",
    "    # Calculate and print the percentage of each class after undersampling\n",
    "    undersampled_class_percentage = df_undersampled['Class'].value_counts(normalize=True) * 100\n",
    "    print(\"\\nPercentage of each class after undersampling:\")\n",
    "    print(undersampled_class_percentage.round(2))\n",
    "\n",
    "    # --- Visualize Class Distribution After Undersampling ---\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.countplot(x='Class', data=df_undersampled, palette='viridis')\n",
    "    plt.title('Class Distribution After Random Undersampling', fontsize=14)\n",
    "    plt.xlabel('Class (0: Non-Fraud, 1: Fraud)', fontsize=12)\n",
    "    plt.ylabel('Number of Transactions', fontsize=12)\n",
    "    plt.xticks(ticks=[0, 1], labels=['Non-Fraud (0)', 'Fraud (1)'])\n",
    "    plt.grid(axis='y', linestyle=':', alpha=0.7)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"Class distribution after random undersampling visualized. The classes are now balanced.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing the Credit Card Fraud Detection dataset: {e}\")\n",
    "    print(\"Please ensure the dataset URL is correct and accessible, and that necessary libraries are installed.\")\n",
    "\n",
    "\n",
    "# --- New Section: Ensuring Consistency Across Training & Inference Datasets: Feature Scaling ---\n",
    "print(\"\\n\\n--- Ensuring Consistency Across Training & Inference Datasets: Feature Scaling ---\")\n",
    "\n",
    "print(\"Generating a synthetic dataset for demonstrating feature scaling...\")\n",
    "\n",
    "# Generate a synthetic dataset\n",
    "# We'll create a simple dataset with two features for demonstration\n",
    "np.random.seed(42) # for reproducibility\n",
    "num_samples = 200\n",
    "data = {\n",
    "    'Feature1': np.random.normal(loc=50, scale=10, size=num_samples),\n",
    "    'Feature2': np.random.normal(loc=1000, scale=200, size=num_samples)\n",
    "}\n",
    "df_scaling = pd.DataFrame(data)\n",
    "\n",
    "print(\"\\nOriginal synthetic dataset (first 5 rows):\")\n",
    "print(df_scaling.head())\n",
    "print(\"\\nOriginal synthetic dataset descriptive statistics:\")\n",
    "print(df_scaling.describe().round(2))\n",
    "\n",
    "# Split the dataset into training and 'new' (inference) data\n",
    "# This simulates having a dataset on which you train your model, and then new data comes in for prediction.\n",
    "X_train, X_new_inference = train_test_split(df_scaling, test_size=0.3, random_state=42)\n",
    "\n",
    "print(f\"\\nTraining data shape: {X_train.shape}\")\n",
    "print(f\"New inference data shape: {X_new_inference.shape}\")\n",
    "\n",
    "print(\"\\n--- Applying StandardScaler to Training Data ---\")\n",
    "# Initialize the StandardScaler\n",
    "# StandardScaler transforms data to have a mean of 0 and a standard deviation of 1.\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "# The .fit() method calculates the mean and standard deviation from the training data.\n",
    "# The .transform() method then applies these calculated values to scale the data.\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Convert the scaled array back to a DataFrame for better readability\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
    "\n",
    "print(\"\\nScaled training data (first 5 rows):\")\n",
    "print(X_train_scaled_df.head())\n",
    "print(\"\\nScaled training data descriptive statistics (should have mean ~0, std ~1):\")\n",
    "print(X_train_scaled_df.describe().round(2))\n",
    "\n",
    "\n",
    "print(\"\\n--- Applying the SAME Scaler to New Inference Data ---\")\n",
    "# Apply the *already fitted* scaler to the new inference data\n",
    "# It is CRUCIAL to use the same scaler (i.e., the one fitted on training data)\n",
    "# to transform new data. This ensures consistency in scaling.\n",
    "X_new_inference_scaled = scaler.transform(X_new_inference)\n",
    "\n",
    "# Convert the scaled array back to a DataFrame\n",
    "X_new_inference_scaled_df = pd.DataFrame(X_new_inference_scaled, columns=X_new_inference.columns)\n",
    "\n",
    "print(\"\\nScaled new inference data (first 5 rows):\")\n",
    "print(X_new_inference_scaled_df.head())\n",
    "print(\"\\nScaled new inference data descriptive statistics:\")\n",
    "print(X_new_inference_scaled_df.describe().round(2))\n",
    "\n",
    "print(\"\\nFeature scaling demonstration complete. Notice that the same scaler object was used for both training and new inference data to maintain consistency.\")\n",
    "print(\"This ensures that your model, trained on scaled data, receives new data in the same expected format.\")\n",
    "\n",
    "\n",
    "# --- New Section: Bias & Fairness in Data: Bias Mitigation Techniques (Reweighing) ---\n",
    "print(\"\\n\\n--- Bias & Fairness in Data: Bias Mitigation Techniques (Reweighing) ---\")\n",
    "\n",
    "# Define the URL for the Adult Income dataset\n",
    "adult_income_url_reweigh = \"https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data\"\n",
    "\n",
    "# Define the column names for the Adult Income dataset\n",
    "column_names_reweigh = [\n",
    "    'age', 'workclass', 'fnlwgt', 'education', 'education-num',\n",
    "    'marital-status', 'occupation', 'relationship', 'race', 'sex',\n",
    "    'capital-gain', 'capital-loss', 'hours-per-week', 'native-country', 'income'\n",
    "]\n",
    "\n",
    "print(\"Loading the Adult Income dataset for bias mitigation...\")\n",
    "try:\n",
    "    df_adult = pd.read_csv(adult_income_url_reweigh, sep=\", \", header=None, names=column_names_reweigh, na_values=[\"?\"], engine='python')\n",
    "    print(\"Adult Income dataset loaded successfully.\")\n",
    "\n",
    "    # Clean the 'sex' and 'income' columns by stripping whitespace\n",
    "    for col in ['sex', 'income']:\n",
    "        if col in df_adult.columns and df_adult[col].dtype == 'object':\n",
    "            df_adult[col] = df_adult[col].str.strip()\n",
    "\n",
    "    # Drop rows with any missing values for a clean demonstration\n",
    "    df_adult.dropna(inplace=True)\n",
    "    print(f\"Dataset shape after dropping missing values: {df_adult.shape}\")\n",
    "\n",
    "    print(\"\\n--- Original Distribution of Income by Gender ---\")\n",
    "    original_distribution = pd.crosstab(df_adult['sex'], df_adult['income'], margins=True, normalize=False)\n",
    "    print(\"Absolute counts:\")\n",
    "    print(original_distribution)\n",
    "\n",
    "    original_percentage = pd.crosstab(df_adult['sex'], df_adult['income'], margins=True, normalize='index') * 100\n",
    "    print(\"\\nRow-wise percentages (Income distribution within each Gender):\")\n",
    "    print(original_percentage.round(2))\n",
    "\n",
    "    # --- Apply Reweighing Technique ---\n",
    "    # Goal: Make the proportion of income classes (especially '>50K') more similar across genders.\n",
    "    # Calculate overall probabilities of income classes\n",
    "    p_y_le50k = df_adult['income'].value_counts(normalize=True)['<=50K']\n",
    "    p_y_gt50k = df_adult['income'].value_counts(normalize=True)['>50K']\n",
    "\n",
    "    # Initialize a new column for weights\n",
    "    df_adult['sample_weight'] = 1.0\n",
    "\n",
    "    # Calculate weights based on the formula: weight(s, y) = P(Y=y) / P(Y=y | S=s)\n",
    "    # This aims to make the joint distribution P(S, Y) proportional to P(S)P(Y)\n",
    "    # effectively making S and Y independent in the weighted dataset.\n",
    "\n",
    "    # Iterate through unique combinations of 'sex' and 'income'\n",
    "    for sex_val in df_adult['sex'].unique():\n",
    "        for income_val in df_adult['income'].unique():\n",
    "            # Probability of income_val given sex_val: P(Y=y | S=s)\n",
    "            p_y_given_s = len(df_adult[(df_adult['sex'] == sex_val) & (df_adult['income'] == income_val)]) / len(df_adult[df_adult['sex'] == sex_val])\n",
    "\n",
    "            # Overall probability of income_val: P(Y=y)\n",
    "            p_y = df_adult['income'].value_counts(normalize=True)[income_val]\n",
    "\n",
    "            # Calculate weight for this specific group\n",
    "            # Avoid division by zero if a group has no samples\n",
    "            if p_y_given_s > 0:\n",
    "                weight = p_y / p_y_given_s\n",
    "            else:\n",
    "                weight = 0 # Assign 0 weight if the group doesn't exist\n",
    "\n",
    "            # Assign calculated weight to the corresponding samples\n",
    "            df_adult.loc[(df_adult['sex'] == sex_val) & (df_adult['income'] == income_val), 'sample_weight'] = weight\n",
    "\n",
    "    print(\"\\n--- Distribution of Income by Gender After Reweighing (Weighted Counts) ---\")\n",
    "    # Calculate weighted counts\n",
    "    weighted_distribution = df_adult.groupby(['sex', 'income'])['sample_weight'].sum().unstack(fill_value=0)\n",
    "    print(\"Weighted counts:\")\n",
    "    print(weighted_distribution)\n",
    "\n",
    "    # Calculate weighted percentages\n",
    "    # Sum of weights for each gender group\n",
    "    total_weight_female = weighted_distribution.loc['Female'].sum()\n",
    "    total_weight_male = weighted_distribution.loc['Male'].sum()\n",
    "\n",
    "    # Calculate weighted percentages within each gender group\n",
    "    weighted_percentage_female = (weighted_distribution.loc['Female'] / total_weight_female) * 100\n",
    "    weighted_percentage_male = (weighted_distribution.loc['Male'] / total_weight_male) * 100\n",
    "\n",
    "    weighted_percentage_df = pd.DataFrame([weighted_percentage_female, weighted_percentage_male], index=['Female', 'Male'])\n",
    "    print(\"\\nRow-wise weighted percentages (Income distribution within each Gender after reweighing):\")\n",
    "    print(weighted_percentage_df.round(2))\n",
    "\n",
    "    print(\"\\n--- Visualizing the Impact of Reweighing ---\")\n",
    "\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "    # Plot original distribution\n",
    "    sns.countplot(x='sex', hue='income', data=df_adult, ax=axes[0], palette='coolwarm')\n",
    "    axes[0].set_title('Original Income Distribution by Gender', fontsize=14)\n",
    "    axes[0].set_xlabel('Gender', fontsize=12)\n",
    "    axes[0].set_ylabel('Number of Samples', fontsize=12)\n",
    "    axes[0].legend(title='Income')\n",
    "    axes[0].grid(axis='y', linestyle=':', alpha=0.7)\n",
    "\n",
    "\n",
    "    # Plot weighted distribution (using the calculated weights for visualization)\n",
    "    # For visualization, we can use a bar plot of the weighted counts\n",
    "    weighted_distribution.plot(kind='bar', ax=axes[1], color=['#66c2a5', '#fc8d62']) # Using different colors\n",
    "    axes[1].set_title('Weighted Income Distribution by Gender', fontsize=14)\n",
    "    axes[1].set_xlabel('Gender', fontsize=12)\n",
    "    axes[1].set_ylabel('Weighted Number of Samples', fontsize=12)\n",
    "    axes[1].legend(title='Income')\n",
    "    axes[1].tick_params(axis='x', rotation=0) # Ensure x-axis labels are not rotated\n",
    "    axes[1].grid(axis='y', linestyle=':', alpha=0.7)\n",
    "\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\nReweighing has been applied. The weighted distributions should show a more balanced representation of income levels across different genders compared to the original distribution.\")\n",
    "    print(\"This technique aims to reduce bias by giving more importance to underrepresented groups during model training.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing the Adult Income dataset for bias mitigation: {e}\")\n",
    "    print(\"Please ensure the dataset URL is correct and accessible, and that necessary libraries are installed.\")\n",
    "\n",
    "\n",
    "# --- New Section: Ensuring Consistency Across Training & Inference Datasets: Pipeline Integration ---\n",
    "print(\"\\n\\n--- Ensuring Consistency Across Training & Inference Datasets: Pipeline Integration ---\")\n",
    "\n",
    "print(\"Generating a synthetic dataset for demonstrating pipeline integration...\")\n",
    "\n",
    "# Generate a synthetic dataset for classification\n",
    "np.random.seed(0) # for reproducibility\n",
    "n_samples = 300\n",
    "X_pipeline = pd.DataFrame({\n",
    "    'Feature_A': np.random.normal(loc=100, scale=20, size=n_samples),\n",
    "    'Feature_B': np.random.normal(loc=5, scale=1.5, size=n_samples)\n",
    "})\n",
    "# Create a binary target variable\n",
    "y_pipeline = (X_pipeline['Feature_A'] + X_pipeline['Feature_B'] * 10 > 150).astype(int)\n",
    "\n",
    "print(\"\\nOriginal synthetic dataset for pipeline (first 5 rows):\")\n",
    "print(X_pipeline.head())\n",
    "print(\"\\nOriginal synthetic dataset target distribution:\")\n",
    "print(y_pipeline.value_counts())\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train_pipeline, X_test_pipeline, y_train_pipeline, y_test_pipeline = train_test_split(\n",
    "    X_pipeline, y_pipeline, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"\\nTraining data shape for pipeline: {X_train_pipeline.shape}\")\n",
    "print(f\"Testing data shape for pipeline: {X_test_pipeline.shape}\")\n",
    "\n",
    "print(\"\\n--- Creating and Training a Scikit-learn Pipeline ---\")\n",
    "\n",
    "# Define the pipeline steps:\n",
    "# 1. StandardScaler: To scale the features\n",
    "# 2. LogisticRegression: A simple classification model\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', StandardScaler()),        # Step 1: Feature Scaling\n",
    "    ('classifier', LogisticRegression()) # Step 2: Classification Model\n",
    "])\n",
    "\n",
    "print(\"\\nPipeline created with steps:\")\n",
    "print(pipeline)\n",
    "\n",
    "# Train the pipeline on the training data\n",
    "# When .fit() is called on the pipeline, it sequentially calls fit_transform()\n",
    "# on all transformers (like StandardScaler) and then fit() on the final estimator.\n",
    "print(\"\\nTraining the pipeline...\")\n",
    "pipeline.fit(X_train_pipeline, y_train_pipeline)\n",
    "print(\"Pipeline training complete.\")\n",
    "\n",
    "print(\"\\n--- Demonstrating Inference with the Trained Pipeline ---\")\n",
    "\n",
    "# Make predictions on the test data using the trained pipeline\n",
    "# When .predict() is called on the pipeline, it sequentially calls transform()\n",
    "# on all transformers and then predict() on the final estimator.\n",
    "# This ensures that the same scaling parameters learned from the training data\n",
    "# are applied to the test data automatically.\n",
    "y_pred_pipeline = pipeline.predict(X_test_pipeline)\n",
    "\n",
    "print(\"\\nFirst 10 actual labels from test set:\", y_test_pipeline.head(10).tolist())\n",
    "print(\"First 10 predicted labels from test set:\", y_pred_pipeline[:10].tolist())\n",
    "\n",
    "# Evaluate the pipeline's performance (optional, but good practice)\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test_pipeline, y_pred_pipeline)\n",
    "print(f\"\\nAccuracy of the pipeline on the test set: {accuracy:.4f}\")\n",
    "\n",
    "print(\"\\nPipeline integration demonstration complete.\")\n",
    "print(\"Using a scikit-learn Pipeline ensures that all preprocessing steps (like scaling) are consistently applied to both training and new inference data.\")\n",
    "print(\"This prevents data leakage and ensures that your model receives data in the exact same format it was trained on.\")\n",
    "\n",
    "# --- New Section: Feature Engineering Best Practices: Handling Text Data ---\n",
    "print(\"\\n\\n--- Feature Engineering Best Practices: Handling Text Data ---\")\n",
    "\n",
    "# Define the URL for the SMS Spam Collection dataset\n",
    "# This dataset contains SMS messages labeled as 'ham' (legitimate) or 'spam'.\n",
    "sms_spam_url = \"https://raw.githubusercontent.com/ageron/handson-ml/master/datasets/spam/smsspamcollection.zip\"\n",
    "\n",
    "print(\"Loading the SMS Spam Collection dataset...\")\n",
    "try:\n",
    "    # The dataset is a zip file containing a tab-separated text file.\n",
    "    # We need to read it directly from the zip.\n",
    "    df_sms = pd.read_csv(sms_spam_url, sep='\\t', header=None, names=['label', 'message'], encoding='latin-1')\n",
    "    print(\"SMS Spam Collection dataset loaded successfully.\")\n",
    "    print(\"\\nFirst 5 rows of the SMS dataset:\")\n",
    "    print(df_sms.head())\n",
    "\n",
    "    print(\"\\n--- Initial Class Distribution (Spam vs. Ham) ---\")\n",
    "    print(df_sms['label'].value_counts())\n",
    "\n",
    "    print(\"\\n--- Text Preprocessing ---\")\n",
    "\n",
    "    # Initialize stemmer and stopwords\n",
    "    stemmer = PorterStemmer()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    def preprocess_text(text):\n",
    "        # 1. Lowercase the text\n",
    "        text = text.lower()\n",
    "        # 2. Remove punctuation and numbers\n",
    "        text = re.sub(r'[^a-z\\s]', '', text)\n",
    "        # 3. Tokenize and remove stop words, then stem\n",
    "        words = text.split()\n",
    "        words = [stemmer.stem(word) for word in words if word not in stop_words]\n",
    "        return ' '.join(words)\n",
    "\n",
    "    # Apply preprocessing to the 'message' column\n",
    "    print(\"Applying text preprocessing (lowercasing, removing punctuation/numbers, stop words, stemming)...\")\n",
    "    df_sms['cleaned_message'] = df_sms['message'].apply(preprocess_text)\n",
    "    print(\"\\nOriginal message vs. Cleaned message (first 5 examples):\")\n",
    "    for i in range(5):\n",
    "        print(f\"Original: {df_sms['message'].iloc[i]}\")\n",
    "        print(f\"Cleaned:  {df_sms['cleaned_message'].iloc[i]}\\n\")\n",
    "\n",
    "    print(\"\\n--- Feature Extraction using TF-IDF ---\")\n",
    "\n",
    "    # Initialize TF-IDF Vectorizer\n",
    "    # max_features limits the number of features (vocabulary size)\n",
    "    # min_df ignores terms that appear in too few documents\n",
    "    # max_df ignores terms that appear in too many documents\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000, min_df=5, max_df=0.8)\n",
    "\n",
    "    # Fit the vectorizer on the cleaned messages and transform them\n",
    "    # This converts text data into numerical TF-IDF features\n",
    "    X_tfidf = tfidf_vectorizer.fit_transform(df_sms['cleaned_message'])\n",
    "\n",
    "    # Convert the TF-IDF matrix to a DataFrame for easier inspection (optional, for small datasets)\n",
    "    # For very large datasets, keep it as a sparse matrix for memory efficiency\n",
    "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "    print(f\"\\nShape of TF-IDF features: {X_tfidf.shape}\")\n",
    "    print(\"\\nFirst 5 rows of TF-IDF features (showing a subset of columns):\")\n",
    "    print(X_tfidf_df.iloc[:5, :10]) # Display first 5 rows and first 10 columns\n",
    "\n",
    "    print(\"\\nTF-IDF feature extraction complete.\")\n",
    "    print(\"These numerical features can now be used to train machine learning models for tasks like spam detection.\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while processing the SMS Spam Collection dataset: {e}\")\n",
    "    print(\"Please ensure the dataset URL is correct and accessible, and that NLTK data (stopwords, punkt) is downloaded.\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
