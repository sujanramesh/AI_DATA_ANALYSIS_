{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI & Machine Learning for Data Quality\n",
    "**Description**: AI and machine learning can automate and enhance data quality checks by learning patterns and identifying anomalies more effectively than static rules.\n",
    "\n",
    "**Task 1**: Training a model to predict and flag unusual trend patterns in sales data that\n",
    "deviate from historical norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Using clustering algorithms to detect duplicate records where entries are not\n",
    "exactly identical."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Implementing classification models to validate data based on learned\n",
    "characteristics from labeled datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write your code from here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error during clustering: empty vocabulary; perhaps the documents only contain stop words\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Missing 'cluster' column",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 139\u001b[0m\n\u001b[1;32m    135\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ All tests passed!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m     \u001b[43mrun_tests\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     main()\n",
      "Cell \u001b[0;32mIn[4], line 122\u001b[0m, in \u001b[0;36mrun_tests\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m test_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mB\u001b[39m\u001b[38;5;124m'\u001b[39m], \u001b[38;5;124m'\u001b[39m\u001b[38;5;124memail\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ma@a.com\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb@b.com\u001b[39m\u001b[38;5;124m'\u001b[39m]})\n\u001b[1;32m    121\u001b[0m clustered \u001b[38;5;241m=\u001b[39m cluster_similar_names(test_data\u001b[38;5;241m.\u001b[39mcopy())\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m clustered\u001b[38;5;241m.\u001b[39mcolumns, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m column\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Test 3: Classifier doesn't return None\u001b[39;00m\n\u001b[1;32m    125\u001b[0m mock_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[1;32m    126\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnull_ratio\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.2\u001b[39m],\n\u001b[1;32m    127\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutlier_count\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m: [\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    131\u001b[0m })\n",
      "\u001b[0;31mAssertionError\u001b[0m: Missing 'cluster' column"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import IsolationForest, RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# --- MODULES / FUNCTIONS --- #\n",
    "\n",
    "def detect_sales_anomalies(df, column='sales', contamination=0.05):\n",
    "    try:\n",
    "        model = IsolationForest(contamination=contamination, random_state=42)\n",
    "        df['anomaly'] = model.fit_predict(df[[column]])\n",
    "        df['is_anomaly'] = df['anomaly'] == -1\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(\"Error during anomaly detection:\", e)\n",
    "        return df\n",
    "\n",
    "\n",
    "def plot_anomalies(df):\n",
    "    try:\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        plt.plot(df['date'], df['sales'], label='Sales')\n",
    "        plt.scatter(df[df['is_anomaly']]['date'], df[df['is_anomaly']]['sales'], color='red', label='Anomaly')\n",
    "        plt.title(\"Sales Data with Anomaly Detection\")\n",
    "        plt.xlabel(\"Date\")\n",
    "        plt.ylabel(\"Sales\")\n",
    "        plt.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    except Exception as e:\n",
    "        print(\"Error during plotting:\", e)\n",
    "\n",
    "\n",
    "def cluster_similar_names(data):\n",
    "    try:\n",
    "        tfidf = TfidfVectorizer()\n",
    "        name_vectors = tfidf.fit_transform(data['name'])\n",
    "        kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "        clusters = kmeans.fit_predict(name_vectors)\n",
    "        data['cluster'] = clusters\n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(\"Error during clustering:\", e)\n",
    "        return data\n",
    "\n",
    "\n",
    "def classify_data_quality(df):\n",
    "    try:\n",
    "        X = df.drop('label', axis=1)\n",
    "        y = df['label']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
    "        print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "        sns.barplot(x=clf.feature_importances_, y=X.columns)\n",
    "        plt.title('Feature Importance')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        return clf\n",
    "    except Exception as e:\n",
    "        print(\"Error during model training or evaluation:\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "# --- MAIN EXECUTION --- #\n",
    "\n",
    "def main():\n",
    "    # 1. Anomaly Detection in Sales Data\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(start='2022-01-01', periods=180)\n",
    "    sales = np.random.normal(loc=200, scale=20, size=len(dates))\n",
    "    sales[::30] += np.random.normal(100, 30, 6)\n",
    "    df_sales = pd.DataFrame({'date': dates, 'sales': sales})\n",
    "    df_sales = detect_sales_anomalies(df_sales)\n",
    "    plot_anomalies(df_sales)\n",
    "\n",
    "    # 2. Duplicate Detection Using Clustering\n",
    "    data = pd.DataFrame({\n",
    "        'name': ['Jon Doe', 'John Doe', 'J. Doe', 'Jane Smith', 'Janet Smith', 'J. Smith', 'Alice Brown', 'Alicia Brown'],\n",
    "        'email': ['jon@example.com', 'john@example.com', 'jd@example.com',\n",
    "                  'jane@example.com', 'janet@example.com', 'js@example.com',\n",
    "                  'aliceb@example.com', 'alicia@example.com']\n",
    "    })\n",
    "    clustered_data = cluster_similar_names(data)\n",
    "    print(\"\\nPotential Duplicate Groups:\")\n",
    "    for c in clustered_data['cluster'].unique():\n",
    "        print(f\"\\nCluster {c}:\\n\", clustered_data[clustered_data['cluster'] == c])\n",
    "\n",
    "    # 3. Data Quality Classification\n",
    "    df_quality = pd.DataFrame({\n",
    "        'null_ratio': np.random.uniform(0, 1, 500),\n",
    "        'outlier_count': np.random.randint(0, 5, 500),\n",
    "        'type_mismatch': np.random.randint(0, 2, 500),\n",
    "        'duplicate_flag': np.random.randint(0, 2, 500),\n",
    "        'label': np.random.choice([0, 1], 500, p=[0.7, 0.3])  # 0: valid, 1: invalid\n",
    "    })\n",
    "    _ = classify_data_quality(df_quality)\n",
    "\n",
    "\n",
    "# --- UNIT TESTS --- #\n",
    "\n",
    "def run_tests():\n",
    "    # Test 1: Anomaly Detection Output\n",
    "    test_df = pd.DataFrame({'sales': [100, 150, 200, 1000, 130, 120]})\n",
    "    result = detect_sales_anomalies(test_df.copy())\n",
    "    assert 'is_anomaly' in result.columns, \"Missing 'is_anomaly' column\"\n",
    "\n",
    "    # Test 2: Clustering Output\n",
    "    test_data = pd.DataFrame({'name': ['A', 'B'], 'email': ['a@a.com', 'b@b.com']})\n",
    "    clustered = cluster_similar_names(test_data.copy())\n",
    "    assert 'cluster' in clustered.columns, \"Missing 'cluster' column\"\n",
    "\n",
    "    # Test 3: Classifier doesn't return None\n",
    "    mock_df = pd.DataFrame({\n",
    "        'null_ratio': [0.1, 0.2],\n",
    "        'outlier_count': [1, 2],\n",
    "        'type_mismatch': [0, 1],\n",
    "        'duplicate_flag': [1, 0],\n",
    "        'label': [0, 1]\n",
    "    })\n",
    "    model = classify_data_quality(mock_df)\n",
    "    assert model is not None, \"Classifier returned None\"\n",
    "\n",
    "    print(\"✅ All tests passed!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_tests()\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
