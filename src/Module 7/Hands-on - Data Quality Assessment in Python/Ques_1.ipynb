{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Checking Null Values for Completeness\n",
    "\n",
    "**Description**: Verify if there are any null values in a dataset, which indicate incomplete data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**: Checking Data Type Validity\n",
    "\n",
    "**Description**: Ensure that columns contain data of expected types, e.g., ages are integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**: Verify Uniqueness of Identifiers\n",
    "\n",
    "**Description**: Check if a dataset has unique identifiers (e.g., emails)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 4: Validate Email Format Using Regex\n",
    "\n",
    "Description: Validate if email addresses in a dataset have the correct format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 5: Check for Logical Age Validity\n",
    "\n",
    "Description: Ensure ages are within a reasonable human range (e.g., 0-120)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 6: Identify and Handle Missing Data\n",
    "\n",
    "Description: Identify missing values in a dataset and impute them using a simple strategy (e.g., mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 7: Detect Duplicates\n",
    "\n",
    "Description: Detect duplicate rows in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 8: Validate Correctness of Numerical Values\n",
    "\n",
    "Description: Ensure numerical columns are within a specified range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 9: Custom Completeness Rule Violation Report\n",
    "\n",
    "Description: Create a report showing which rows violate specific completeness rules, such as mandatory fields being empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task 10: Advanced Regex for Data Validity Check\n",
    "\n",
    "Description: Check for validity with advanced regex patterns, such as validating complex fields with multi-level rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Task 1: Checking Null Values for Completeness ---\n",
      "Error: customer_data.csv not found. Please ensure it's in the same directory.\n",
      "Skipping Task 1 due to data loading error.\n",
      "\n",
      "--- Task 2: Checking Data Type Validity ---\n",
      "Error: customer_data.csv not found. Please ensure it's in the same directory.\n",
      "Skipping Task 2 due to data loading error.\n",
      "\n",
      "--- Task 3: Verify Uniqueness of Identifiers ---\n",
      "Error: customer_data.csv not found. Please ensure it's in the same directory.\n",
      "Skipping Task 3 due to data loading error.\n",
      "\n",
      "--- Task 4: Validate Email Format Using Regex ---\n",
      "Error: customer_data.csv not found. Please ensure it's in the same directory.\n",
      "Skipping Task 4 due to data loading error or missing 'email' column.\n",
      "\n",
      "--- Task 5: Check for Logical Age Validity ---\n",
      "Error: customer_data.csv not found. Please ensure it's in the same directory.\n",
      "Skipping Task 5 due to data loading error or missing 'age' column.\n",
      "\n",
      "--- Task 6: Identify and Handle Missing Data (Impute with Mean) ---\n",
      "Error: customer_data.csv not found. Please ensure it's in the same directory.\n",
      "Skipping Task 6 due to data loading error.\n",
      "\n",
      "--- Task 7: Detect Duplicates ---\n",
      "Error: customer_data.csv not found. Please ensure it's in the same directory.\n",
      "Skipping Task 7 due to data loading error.\n",
      "\n",
      "--- Task 8: Validate Correctness of Numerical Values ---\n",
      "Error: sales_data.csv not found. Please ensure it's in the same directory.\n",
      "Skipping Task 8 due to data loading error.\n",
      "\n",
      "--- Task 9: Custom Completeness Rule Violation Report ---\n",
      "Error: customer_data.csv not found. Please ensure it's in the same directory.\n",
      "Skipping Task 9 due to data loading error.\n",
      "\n",
      "--- Task 10: Advanced Regex for Data Validity Check ---\n",
      "Error: product_catalog.csv not found. Please ensure it's in the same directory.\n",
      "Skipping Task 10 due to data loading error.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np # For numerical operations like mean imputation\n",
    "\n",
    "# --- Configuration ---\n",
    "CUSTOMER_DATA_PATH = 'customer_data.csv'\n",
    "SALES_DATA_PATH = 'sales_data.csv'\n",
    "PRODUCT_CATALOG_PATH = 'product_catalog.csv'\n",
    "\n",
    "# --- Utility Function to Load Data ---\n",
    "def load_data(file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(file_path)\n",
    "        print(f\"\\nSuccessfully loaded {file_path}\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {file_path} not found. Please ensure it's in the same directory.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame on error\n",
    "\n",
    "# --- Task 1: Checking Null Values for Completeness ---\n",
    "print(\"--- Task 1: Checking Null Values for Completeness ---\")\n",
    "customer_df_t1 = load_data(CUSTOMER_DATA_PATH)\n",
    "if not customer_df_t1.empty:\n",
    "    print(\"Null values per column:\")\n",
    "    print(customer_df_t1.isnull().sum())\n",
    "else:\n",
    "    print(\"Skipping Task 1 due to data loading error.\")\n",
    "\n",
    "\n",
    "# --- Task 2: Checking Data Type Validity ---\n",
    "print(\"\\n--- Task 2: Checking Data Type Validity ---\")\n",
    "customer_df_t2 = load_data(CUSTOMER_DATA_PATH)\n",
    "if not customer_df_t2.empty:\n",
    "    print(\"Current data types:\")\n",
    "    print(customer_df_t2.dtypes)\n",
    "\n",
    "    # Expected data types\n",
    "    expected_dtypes = {\n",
    "        'customer_id': 'object', # or 'string' in newer pandas\n",
    "        'name': 'object',\n",
    "        'age': 'int64',\n",
    "        'email': 'object',\n",
    "        'city': 'object',\n",
    "        'phone_number': 'object',\n",
    "        'registration_date': 'object' # Will convert to datetime below\n",
    "    }\n",
    "\n",
    "    # Attempt to convert 'age' to numeric, coercing errors\n",
    "    customer_df_t2['age_numeric'] = pd.to_numeric(customer_df_t2['age'], errors='coerce')\n",
    "    non_numeric_ages = customer_df_t2[customer_df_t2['age_numeric'].isnull() & customer_df_t2['age'].notnull()]\n",
    "    if not non_numeric_ages.empty:\n",
    "        print(\"\\nRows with non-numeric 'age' values:\")\n",
    "        print(non_numeric_ages[['customer_id', 'age']])\n",
    "    else:\n",
    "        print(\"\\nAll 'age' values are numeric or null.\")\n",
    "\n",
    "    # Attempt to convert 'registration_date' to datetime, coercing errors\n",
    "    customer_df_t2['registration_date_parsed'] = pd.to_datetime(customer_df_t2['registration_date'], errors='coerce')\n",
    "    invalid_dates = customer_df_t2[customer_df_t2['registration_date_parsed'].isnull() & customer_df_t2['registration_date'].notnull()]\n",
    "    if not invalid_dates.empty:\n",
    "        print(\"\\nRows with invalid 'registration_date' format:\")\n",
    "        print(invalid_dates[['customer_id', 'registration_date']])\n",
    "    else:\n",
    "        print(\"\\nAll 'registration_date' values are valid datetime formats or null.\")\n",
    "\n",
    "    print(\"\\nData types after attempting type conversions:\")\n",
    "    print(customer_df_t2.dtypes)\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Task 2 due to data loading error.\")\n",
    "\n",
    "\n",
    "# --- Task 3: Verify Uniqueness of Identifiers ---\n",
    "print(\"\\n--- Task 3: Verify Uniqueness of Identifiers ---\")\n",
    "customer_df_t3 = load_data(CUSTOMER_DATA_PATH)\n",
    "if not customer_df_t3.empty:\n",
    "    # Check uniqueness of 'customer_id'\n",
    "    is_customer_id_unique = customer_df_t3['customer_id'].is_unique\n",
    "    print(f\"Is 'customer_id' unique? {is_customer_id_unique}\")\n",
    "    if not is_customer_id_unique:\n",
    "        duplicate_customer_ids = customer_df_t3[customer_df_t3.duplicated(subset=['customer_id'], keep=False)]\n",
    "        print(\"Duplicate 'customer_id' entries:\")\n",
    "        print(duplicate_customer_ids[['customer_id', 'name', 'email']].sort_values('customer_id'))\n",
    "\n",
    "    # Check uniqueness of 'email' (assuming email should be unique for a customer)\n",
    "    # Exclude nulls as they aren't truly duplicate values to check for uniqueness\n",
    "    unique_emails_count = customer_df_t3['email'].dropna().nunique()\n",
    "    total_non_null_emails = customer_df_t3['email'].dropna().count()\n",
    "    if unique_emails_count != total_non_null_emails:\n",
    "        print(f\"\\n'email' is NOT unique (found {total_non_null_emails - unique_emails_count} duplicate email values excluding nulls).\")\n",
    "        duplicate_emails = customer_df_t3[customer_df_t3.duplicated(subset=['email'], keep=False) & customer_df_t3['email'].notnull()]\n",
    "        print(\"Duplicate 'email' entries:\")\n",
    "        print(duplicate_emails[['customer_id', 'name', 'email']].sort_values('email'))\n",
    "    else:\n",
    "        print(\"\\n'email' is unique (excluding nulls).\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Task 3 due to data loading error.\")\n",
    "\n",
    "\n",
    "# --- Task 4: Validate Email Format Using Regex (Revisited with dedicated task focus) ---\n",
    "print(\"\\n--- Task 4: Validate Email Format Using Regex ---\")\n",
    "customer_df_t4 = load_data(CUSTOMER_DATA_PATH)\n",
    "if not customer_df_t4.empty and 'email' in customer_df_t4.columns:\n",
    "    email_regex = r\"^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$\"\n",
    "    def validate_email(email):\n",
    "        if pd.isna(email): return False # Consider missing emails as invalid format for this check\n",
    "        return bool(re.match(email_regex, str(email)))\n",
    "\n",
    "    customer_df_t4['email_format_valid'] = customer_df_t4['email'].apply(validate_email)\n",
    "    invalid_format_emails = customer_df_t4[customer_df_t4['email_format_valid'] == False]\n",
    "\n",
    "    if not invalid_format_emails.empty:\n",
    "        print(\"\\nRecords with invalid email format:\")\n",
    "        print(invalid_format_emails[['customer_id', 'email']])\n",
    "        print(f\"Total records with invalid email format: {len(invalid_format_emails)}\")\n",
    "    else:\n",
    "        print(\"\\nAll emails adhere to the specified format.\")\n",
    "else:\n",
    "    print(\"Skipping Task 4 due to data loading error or missing 'email' column.\")\n",
    "\n",
    "\n",
    "# --- Task 5: Check for Logical Age Validity ---\n",
    "print(\"\\n--- Task 5: Check for Logical Age Validity ---\")\n",
    "customer_df_t5 = load_data(CUSTOMER_DATA_PATH)\n",
    "if not customer_df_t5.empty and 'age' in customer_df_t5.columns:\n",
    "    min_age = 0\n",
    "    max_age = 120\n",
    "\n",
    "    # Ensure 'age' is numeric for comparison, coercing errors\n",
    "    customer_df_t5['age_numeric'] = pd.to_numeric(customer_df_t5['age'], errors='coerce')\n",
    "\n",
    "    # Identify ages outside the reasonable range (and non-numeric ones, as they become NaN)\n",
    "    invalid_age_range = customer_df_t5[\n",
    "        (customer_df_t5['age_numeric'] < min_age) | (customer_df_t5['age_numeric'] > max_age) |\n",
    "        (customer_df_t5['age_numeric'].isna() & customer_df_t5['age'].notnull()) # Catch non-numeric that coerced to NaN\n",
    "    ]\n",
    "\n",
    "    if not invalid_age_range.empty:\n",
    "        print(f\"\\nRecords with age outside the logical range ({min_age}-{max_age}) or non-numeric:\")\n",
    "        print(invalid_age_range[['customer_id', 'name', 'age']])\n",
    "        print(f\"Total invalid age values found: {len(invalid_age_range)}\")\n",
    "    else:\n",
    "        print(\"\\nAll age values are within the logical range.\")\n",
    "else:\n",
    "    print(\"Skipping Task 5 due to data loading error or missing 'age' column.\")\n",
    "\n",
    "\n",
    "# --- Task 6: Identify and Handle Missing Data (Impute with Mean) ---\n",
    "print(\"\\n--- Task 6: Identify and Handle Missing Data (Impute with Mean) ---\")\n",
    "customer_df_t6 = load_data(CUSTOMER_DATA_PATH)\n",
    "if not customer_df_t6.empty:\n",
    "    print(\"Original null values:\")\n",
    "    print(customer_df_t6.isnull().sum())\n",
    "\n",
    "    # Example: Impute missing 'age' with the mean age\n",
    "    # First, ensure 'age' is numeric\n",
    "    customer_df_t6['age_numeric'] = pd.to_numeric(customer_df_t6['age'], errors='coerce')\n",
    "    mean_age = customer_df_t6['age_numeric'].mean()\n",
    "    print(f\"\\nMean age for imputation: {mean_age:.2f}\")\n",
    "\n",
    "    customer_df_imputed = customer_df_t6.copy() # Work on a copy\n",
    "    customer_df_imputed['age_numeric'].fillna(round(mean_age), inplace=True) # Impute age\n",
    "\n",
    "    # Example: Impute missing 'city' with a placeholder\n",
    "    customer_df_imputed['city'].fillna('Unknown', inplace=True)\n",
    "\n",
    "    # Example: Impute missing 'phone_number' with a placeholder\n",
    "    customer_df_imputed['phone_number'].fillna('N/A', inplace=True)\n",
    "\n",
    "    print(\"\\nNull values after imputation:\")\n",
    "    print(customer_df_imputed.isnull().sum())\n",
    "    print(\"\\nSample of DataFrame after imputation (first 5 rows):\")\n",
    "    print(customer_df_imputed.head())\n",
    "else:\n",
    "    print(\"Skipping Task 6 due to data loading error.\")\n",
    "\n",
    "\n",
    "# --- Task 7: Detect Duplicates ---\n",
    "print(\"\\n--- Task 7: Detect Duplicates ---\")\n",
    "customer_df_t7 = load_data(CUSTOMER_DATA_PATH)\n",
    "if not customer_df_t7.empty:\n",
    "    # Detect exact duplicate rows\n",
    "    duplicate_rows = customer_df_t7[customer_df_t7.duplicated(keep=False)] # keep=False marks all duplicates\n",
    "    \n",
    "    if not duplicate_rows.empty:\n",
    "        print(\"Detected duplicate rows (exact matches):\")\n",
    "        print(duplicate_rows.sort_values(by=list(customer_df_t7.columns)))\n",
    "        print(f\"Total duplicate rows found: {len(duplicate_rows)}\")\n",
    "    else:\n",
    "        print(\"No exact duplicate rows found.\")\n",
    "\n",
    "    # Detect duplicates based on a subset of columns (e.g., customer_id and email might imply same customer)\n",
    "    # Note: 'customer_id' already handled in Task 3 for uniqueness, but this demonstrates subset checking\n",
    "    duplicate_id_email = customer_df_t7[customer_df_t7.duplicated(subset=['customer_id', 'email'], keep=False)]\n",
    "    if not duplicate_id_email.empty:\n",
    "        print(\"\\nDetected rows with duplicate 'customer_id' and 'email' combination:\")\n",
    "        print(duplicate_id_email.sort_values(by=['customer_id', 'email']))\n",
    "    else:\n",
    "        print(\"\\nNo duplicate 'customer_id' and 'email' combinations found.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Task 7 due to data loading error.\")\n",
    "\n",
    "\n",
    "# --- Task 8: Validate Correctness of Numerical Values ---\n",
    "print(\"\\n--- Task 8: Validate Correctness of Numerical Values ---\")\n",
    "sales_df_t8 = load_data(SALES_DATA_PATH)\n",
    "if not sales_df_t8.empty:\n",
    "    # Validate 'sales_amount': must be positive\n",
    "    invalid_sales_amount = sales_df_t8[pd.to_numeric(sales_df_t8['sales_amount'], errors='coerce') <= 0]\n",
    "    if not invalid_sales_amount.empty:\n",
    "        print(\"\\nRecords with non-positive 'sales_amount':\")\n",
    "        print(invalid_sales_amount[['transaction_id', 'sales_amount']])\n",
    "    else:\n",
    "        print(\"\\nAll 'sales_amount' values are positive.\")\n",
    "\n",
    "    # Validate 'quantity': must be positive integer\n",
    "    invalid_quantity = sales_df_t8[pd.to_numeric(sales_df_t8['quantity'], errors='coerce') <= 0]\n",
    "    if not invalid_quantity.empty:\n",
    "        print(\"\\nRecords with non-positive 'quantity':\")\n",
    "        print(invalid_quantity[['transaction_id', 'quantity']])\n",
    "    else:\n",
    "        print(\"\\nAll 'quantity' values are positive.\")\n",
    "\n",
    "    # Validate 'discount_rate': must be between 0 and 1 (inclusive)\n",
    "    invalid_discount_rate = sales_df_t8[\n",
    "        (pd.to_numeric(sales_df_t8['discount_rate'], errors='coerce') < 0) |\n",
    "        (pd.to_numeric(sales_df_t8['discount_rate'], errors='coerce') > 1)\n",
    "    ]\n",
    "    if not invalid_discount_rate.empty:\n",
    "        print(\"\\nRecords with 'discount_rate' outside 0-1 range:\")\n",
    "        print(invalid_discount_rate[['transaction_id', 'discount_rate']])\n",
    "    else:\n",
    "        print(\"\\nAll 'discount_rate' values are within the 0-1 range.\")\n",
    "else:\n",
    "    print(\"Skipping Task 8 due to data loading error.\")\n",
    "\n",
    "\n",
    "# --- Task 9: Custom Completeness Rule Violation Report ---\n",
    "print(\"\\n--- Task 9: Custom Completeness Rule Violation Report ---\")\n",
    "customer_df_t9 = load_data(CUSTOMER_DATA_PATH)\n",
    "if not customer_df_t9.empty:\n",
    "    # Define mandatory fields\n",
    "    mandatory_fields = ['name', 'email', 'customer_id']\n",
    "\n",
    "    # Create a boolean Series indicating if any mandatory field is null for each row\n",
    "    customer_df_t9['violates_completeness_rule'] = customer_df_t9[mandatory_fields].isnull().any(axis=1)\n",
    "\n",
    "    # Filter for rows that violate the rule\n",
    "    violations_report = customer_df_t9[customer_df_t9['violates_completeness_rule']].copy()\n",
    "\n",
    "    if not violations_report.empty:\n",
    "        print(\"\\n--- Completeness Rule Violation Report ---\")\n",
    "        print(\"The following customer profiles violate mandatory field completeness rules:\")\n",
    "        # For each violating row, identify exactly which fields are missing\n",
    "        violations_report['missing_fields'] = violations_report.apply(\n",
    "            lambda row: [col for col in mandatory_fields if pd.isna(row[col])], axis=1\n",
    "        )\n",
    "        print(violations_report[['customer_id', 'name', 'email', 'missing_fields']])\n",
    "        print(f\"\\nTotal profiles violating completeness rules: {len(violations_report)}\")\n",
    "    else:\n",
    "        print(\"\\nNo profiles violate the mandatory field completeness rules.\")\n",
    "else:\n",
    "    print(\"Skipping Task 9 due to data loading error.\")\n",
    "\n",
    "\n",
    "# --- Task 10: Advanced Regex for Data Validity Check ---\n",
    "print(\"\\n--- Task 10: Advanced Regex for Data Validity Check ---\")\n",
    "product_df_t10 = load_data(PRODUCT_CATALOG_PATH)\n",
    "if not product_df_t10.empty:\n",
    "    # Rule 1: Validate 'product_code' format: e.g., 'PROD-XYZ-V1.0' or 'ITEM-123-BETA'\n",
    "    # Pattern: (PROD|ITEM)-[A-Z0-9]{3,}-[A-Z0-9.]{3,}\n",
    "    # This allows 'PROD' or 'ITEM' prefix, followed by a hyphen, then 3+ alphanumeric,\n",
    "    # then a hyphen, then 3+ alphanumeric/dot characters.\n",
    "    product_code_regex = r\"^(PROD|ITEM)-[A-Z0-9]{3,}-[A-Z0-9.]{3,}$\"\n",
    "    def validate_product_code(code):\n",
    "        if pd.isna(code): return False\n",
    "        return bool(re.match(product_code_regex, str(code)))\n",
    "\n",
    "    product_df_t10['product_code_valid'] = product_df_t10['product_code'].apply(validate_product_code)\n",
    "\n",
    "    # Rule 2: Validate 'version' format: e.g., '1.0', '2.1', '0.9' (numeric.numeric)\n",
    "    # Pattern: ^\\d+\\.\\d+$ (one or more digits, dot, one or more digits)\n",
    "    version_regex = r\"^\\d+\\.\\d+$\"\n",
    "    def validate_version(version):\n",
    "        if pd.isna(version): return False\n",
    "        return bool(re.match(version_regex, str(version)))\n",
    "\n",
    "    product_df_t10['version_valid'] = product_df_t10['version'].apply(validate_version)\n",
    "\n",
    "    print(\"\\nAdvanced Regex Validation Results:\")\n",
    "    print(product_df_t10[['product_code', 'product_code_valid', 'version', 'version_valid']])\n",
    "\n",
    "    invalid_product_codes = product_df_t10[product_df_t10['product_code_valid'] == False]\n",
    "    if not invalid_product_codes.empty:\n",
    "        print(\"\\nProducts with invalid 'product_code' format:\")\n",
    "        print(invalid_product_codes[['product_code', 'product_name']])\n",
    "    else:\n",
    "        print(\"\\nAll product codes adhere to the specified format.\")\n",
    "\n",
    "    invalid_versions = product_df_t10[product_df_t10['version_valid'] == False]\n",
    "    if not invalid_versions.empty:\n",
    "        print(\"\\nProducts with invalid 'version' format:\")\n",
    "        print(invalid_versions[['product_code', 'version']])\n",
    "    else:\n",
    "        print(\"\\nAll versions adhere to the specified format.\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping Task 10 due to data loading error.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
